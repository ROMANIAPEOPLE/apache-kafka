# 아파치 카프카 심화개념

### Kafka 가 파일시스템에 저장되는 과정

********1. zero-copy********

kafka 는 기본적으로 메모리 기반으로 운영되는 다른 message queue 들과 달리 disk 기반으로 운영이 된다.

디스크 기반으로 운영이 되기 때문에 대량의 메시지를 보고나할 수 있고, 예측 불가능한 서버 다운에도 대처할 수 있는 장점이 있다. 그런데, 디스크 기반이면 당연히 메모리 기반보다 느려야 하는데 왜 빠를까?

예로부터, 데이터를 디스크에서 읽어 네트워크로 전송하는 작업은 아래와 같이 4번의 카피와 4번의 컨텍스트 스위칭을 요구한다.

- 디스크에서 read buffer 로
- read buffer 에서 application buffer 로
- application buffer 에서 socket buffer 로
- socket buffer 에서 nic buffer 로

그러나, kafka 는 아래와 같이 동작한다.

- 디스크에서 read buffer 로 (기존 방식과 동일)
- read buffer 에서 socket buffer 로 ( application buffer 로 이동하는 과정을 생략)
- socket buffer 에서 nic buffer 로

즉, 이렇게 copy 횟수와 컨텍스트 스위칭 횟수가 줄어들게 되었다. kafka 가 바로 이와 같은 zero-copy 방식으로 동작하기 때문이다.

********2. OS 의 페이지 캐시 사용********

운영체제는 application 이 사용하는 부분을 메모리에 할당하고, 남은 잔여 메모리 일부를 페이지 캐시로 유지해 Disk 에 I/O 하는 작업을 최소화 시킨다.

Kafka 는 페이지 캐시를 사용해서 빠른 처리를 지원한다.

- producer 가 broker 에 레코드를 send() 한다.
- 브로커는 프로듀서로 부터 전달 받은 레코드를 바로 disk 로 저장하는 것이 아닌, OS의 페이지 캐시에 데이터를 저장함.
- 이후 flush 가 일어나면 OS 의 page cache 에서 disk 로 데이터가 저장됨.

→ BUT, 페이지 캐시에서 disk 로 데이터가 넘어가기 전에 장애가 발생한다면 데이터가 유실될 수 있음. 그래서 항상 replica factor 를 구성해야함.

---

### ISR (In - Sync - Replicas)

리더파티션와 팔로워 파티션의 정합성이 일치하는 상태의 파티션을 ISR 파티션이라고 부른다.

정확히는, HW(high watermark) 까지 복제가 완료된 파티션을 ISR 파티션이라고 부르며, hw 란 리더 파티션과 팔로워 파티션 간에 복제된 Offset 의 위치를 의미한다.

<img width="648" alt="스크린샷 2023-09-10 오후 9 15 35" src="https://github.com/ROMANIAPEOPLE/apache-kafka/assets/39195377/258b0c27-4e4b-43d7-8417-644a9de86781">



즉, LEO (Log End Offset) 까지는 복제가 되지 않은 상태여도, hw 까지만 복제가 되었다면 ISR 이다.

→ 안전하게 사용하기 위해 (고가용성) ISR 에 성공적으로 복제된 지점을 나타내는 high watermark 의 레코드 까지만 소비를 하는 것이다.

******************************ISR 상태의 팔로워 파티션이 죽으면?******************************

→ 리더 파티션은  해당 파티션을 ISR 리스트에서 제거한다.

**************************************************************************************만약 ISR 상태에서 OSR 로 변경되면 ?**************************************************************************************

→ 리더 파티션은 변경된 파티션의 정보를 주키퍼로 전달한다. (메타데이터 관리를 위해)

****************************************************************만약 리더파티션이 죽어서 팔로워 파티션이 리더가 되어야 한다면?****************************************************************

→ 주키퍼가 먼저 해당 브로커 (리더 파티션이 존재하는)의 장애를 감지하고, 그룹 코디네이터 (브로커 중 하나) 는 새로운 리더와 ISR 상태의 팔로워를 선출한 후 주키퍼로 정보들을 전달한다. 그리고 주키퍼는 또 모든 브로커들에게 그 정보들을 알린다.

<img width="657" alt="스크린샷 2023-09-10 오후 9 16 41" src="https://github.com/ROMANIAPEOPLE/apache-kafka/assets/39195377/f0ae5085-fb26-439d-bcfc-34a2c1a020d5">


**********ISR 과 OSR 을 판단하는 두 가지 옵션**********

1. `replica.lag.max.messages` 
    
    리더 파티션의 HW 과 위 옵션에 설정한 N 개 만큼 차이가 난다면 OSR 로 간주한다.
    
    그러나, 이 방식에는 문제점이 존재한다. 갑자기 메시지 유입량이 많아져서 일시적으로 복제가 늦어지는 경우에도 OSR 로 판단해 버릴수도 있기 때문이다.
    → 팔로워 파티션 입장 : 정상적으로 작동 하다가 단지 잠깐의 지연이 발생했을 뿐인데…
    
2. `[replica.lag.time.max.ms](http://replica.lag.time.max.ms)`  
팔로워가 리더로 Fetch 요청을 보내는 Interval 을 체크한다. 참고로 Fetch 란 팔로워 파티션이 리더 파티션의 offset 을 복제하는 행위를 의미한다. 만약 설정한 시간 동안 Fetch 요청이 오지 않는다면 OSR 로 판단한다.
→ 예를 들어, 해당 옵션을 10000 으로 설정했다면 팔로워 파티션은 리더에 fetch 요청을 10초 안에만 보내면 된다.

******************참고로, Kafka 창시자가 창업한 회사에서 사용하는 Confluent Kafka 에서는 1번 옵션 자체를 제거해버렸다. (안정적인 운영을 위해서는 2번 옵션을 사용할 것.)******************

---

### 파티션의 배치 (batch 아님)

<img width="676" alt="스크린샷 2023-09-10 오후 9 16 55" src="https://github.com/ROMANIAPEOPLE/apache-kafka/assets/39195377/e0ee5c79-6cac-4fbc-bfe1-5ceadd95b5bd">



각 토픽의 파티션은 통상적으로 위 그림과 같이 배치된다.

동일한 토픽의 리더 파티션들은 하나의 브로커로 집중되지 않도록 해야하며, 특정 브로커에 리더 파티션이 쏠리는 것을 방지하는 것이 카프카 클러스터를 운영할 때 중요한 부분이다.

---

### 프로듀서

프로듀서가 브로커로 레코드를 보내는 과정을 잘 알아야 한다.

1. 프로듀서에서 레코드를 send() 하면 어큐뮬레이터 라는 곳에 해당 레코드가 쌓이게 된다.
2. 이 레코드는 batch 단위로 브로커에 전송되는데, 건바이건으로 전송하지 않는 이유는 프로듀서와 브로커 사이의 TCP 통신을 최소화 하기 위해서다.

**********************************************************************************************************************프로듀서 → 어큐뮬레이터 → 브로커의 배치 처리**********************************************************************************************************************

위 과정을 거쳐서 브로커로 레코드가 적재되는데, 2개의 옵션을 잘 설정해야 한다.

- [linger.ms](http://linger.ms) : 프로듀서가 어큐뮬레이터로 레코드를 전송할 때 해당 옵션에 설정한 시간 만큼 기다렸다가 Sender() 메소드를 실행한다.
- batch.size : 프로듀서가 어큐뮬레이터로 레코드를 전송할 때 해당 옵션에 설정한 size 만큼 채워지면 전송한다.

그런데 대부분은 첫 번째 옵션을 함께 사용하거나 첫 번째 옵션만 사용한다. 그 이유는 batch size 만큼 채워질때 까지 기다리면 브로커로 전송되는 interval 이 너무 길어질 수도 있기 때문이다.

************************************************프로듀서가 브로커로 메시지를 전송할 때, KEY 가 있으면 어떻게 전송할까?************************************************

만약 KEY 를 추가해서 메시지를 보내게 되면 파티셔너 설정과 무관하게 다음과 같은 공식으로 파티션에 적재하게 된다.

`KEY 값을 hash 값으로 변환 후, 파티션의 개수로 나눗셈 나누기(%)` 를 한 파티션에 보낸다.

→ 그렇기 때문에 만약 파티션을 중간에 추가하게 되면 이 공식이 깨지면서 순서 보장도 깨지게 된다.

---

### Acks 옵션

프로듀서가 브로커오 레코드를 전송할 때 성공 여부를 어디까지 확인을 해야할까.

1. Acks 0 : 프로듀서는 레코드를 전송 후 성공/실패 여부를 신경쓰지 않는다. 정확히 말하면 무조건 성공했다고 가정하기 때문에 실패로 인한 일부 데이터 유실을 retry 할 수 없다. 
2. Acks 1 : 프로듀서가 브로커로 레코드를 전송하면 리더 파티션에 정상적으로 적재가 되었는지 여부만 확인한다. 팔로워 파티션들이 정상적으로 Fetch 를 날려 복제가 완료 되었는지는 신경쓰지 않는다.
    
    ****************acks 1 에서의 문제점.****************
    
    - acks 1 은 at most once 로 최대 1번 성공을 보장해줌.
    - 리더파티션에는 (m1,m2,m3,m4) 의 레코드가 존재하고, hw 는 m3 이다.
    - 팔로워 파티션 중 하나가 ISR 상태 (즉, m1,m2,m3 까지 복제가 완료된 상태)
    - 리더 파티션에 장애가 발생하게 되면 ISR 팔로워 파티션이 리더가됨.
    - 팔로워에서 리더로 승격한 파티션에는 (m1,m2,m3) 의 레코드가 존재
    - acks 1의 옵션으로 인해 기존 리더파티션에 m4 까지 정상적으로 발행되었기 때문에 retry 하지 않음
    - m4 레코드 영구 유실.
3. Acks -1 (또는 all 이라고도 부름) : 리더 파티션 적재 + 팔로워 파티션 복제 까지 확인
    - min.insync.replicas 옵션을 함께 설정해서 사용하는 경우가 많은데, 해당 옵션에 설정한 수 만큼 파티션들을 확인한다. (즉, 2로 설정했으면 리더파티션 + 팔로워파티션1개 확인. 대부분 2로 설정)
    - 처리량이 너무 낮아서 실사용에 무리가 있음
    - at least once 보장

### 카프카의 순서 보장

동일한 KEY 를 사용하는 경우, 파티션의 개수와 컨슈머의 개수가 동일한 경우 동일한 키에 대해서는 순서를 보장할 수 있다.

그러나, 파티션의 개수가 더 많고 컨슈머의 개수가 적은 경우에는 순서 보장이 어렵다.

그 이유는, 파티션의 개수가 더 많은 경우 한 개의 컨슈머는 두 개의 파티션에서 데이터를 가져가야 하는데 파티션이 컨슈머에게 주는 데이터 처리 속도가 네트워크 등의 이슈로 차이가 날 수 있기 때문이다.

즉, 순서를 보장하고 싶은 경우 파티션과 컨슈머의 개수를 1:1 로 맞추도록 하자.

또한, 배치 사이즈로 프로듀서에서 브로커에 레코드를 적재하는 경우에도 순서 보장에 문제가 생길수도 있다. (멱등성 설정 필요)

---

### Consumer 의 어싸이너

partition 과 consumer 를 어떤 방식으로 매칭 시킬지 결정

- Range Assignor 
무조건 순서대로 할당해준다. kafka 2.5.0 버전 부터 default 로 설정되어 있다.
    
<img width="671" alt="스크린샷 2023-09-10 오후 9 17 24" src="https://github.com/ROMANIAPEOPLE/apache-kafka/assets/39195377/e6491e97-827d-4dc4-a87a-3701220a11dd">

    
- Round Robin Assignor 
Range 보다 효율적으로 분배하여 할당한다. 단, 할당 불균형 발생 가능성 높음 
만약 컨슈머 리밸런싱이 일어나면 모든 파티션과 컨슈머를 재할당함.
- Sticky Assignor 
Round Robin 의 할당 불균형을 개선한 방식. 만약 consumer 리밸런싱이 일어나게 되면 기존 할당을 유지하면서 나머지 부분만 재발당 함.
- Cooperative Sticky Assignor  (따로 정리)

### Cooperative Sticky Assignor

**************Eager 리밸런싱 프로토콜 (지금까지 사용되었던 방식)**************

1. 각 컨슈머 구성원들이 그룹 코디네이터로 JoinGroup 요청을 보낸다.
2. 요청을 보내면 모든 파티션들은 revoked 가 된다.
3. 그리고 다시 SyncGroup 요청을 브로커로 보내고, 그 이후에 다시 파티션을 각 컨슈머에 분배한다.

→ 2번 과정부터 revoked 가 되기 때문에 stop-the-world 가 발생한다. (consumer 들이 데이터를 컨슈밍 할 수 없게 된다.)

************************************************************************************************Incremental Cooperative Rebalancing 프로토콜 (2.5.0 버전에서 추가)************************************************************************************************


위 그림처럼 Consumer C 가 추가되었을 때 컨슈머가 리벨런싱 되는 가장 이상적인 방법은 Consumer A 의 3번 파티션을 Consumer C 에 할당하고, Consumer B 는 그대로 두는 것이다. 이러한 이상적인 매커니즘을 구현한 방법이 해당 프로토콜이다.

1. 각 컨슈머 구성원들이 JoinGroup 요청을 보낸다.
2. 브로커느 요청에 대한 응답을 하고, 다시 SyncGroup 요청을 브로커로 보낸다.
3. SyncGroup 에 대한 응답을 각 컨슈머로 보내는데, 이 때 revoked 시킬 partition 정보를 포함해서 보낸다.
4. revoked 된 파티션은 다시 JoinGroup 요청을 보낸댜.
5. 브로커느 요청에 대한 응답을 하고, 다시 SyncGroup 요청을 브로커로 보낸다.
6. SyncGroup 에 대한 응답을 각 컨슈머로 보내는데, 이 때 revoked 된 파티션이 join 할 컨슈머 정보를 보낸다.
7. 파티션이 컨슈머로 어싸인 된다.

즉, 1~3 번 과정에서 리벨런싱이 1번, 4~6번 과정에서 리밸런싱이 1번 해서 총 2번의 리밸런싱이 이루어진다. (대신 stop-the-world 가 없다.)

<img width="704" alt="스크린샷 2023-09-10 오후 9 17 45" src="https://github.com/ROMANIAPEOPLE/apache-kafka/assets/39195377/3e1eb182-736e-4a6d-abdd-9242f3d9ed31">


---

### Delivery Semantics

1. At-Most-Once (최대 한번) : 전달되지 않을수도 있음 (Acks 0 또는 1)
    1. 확인 시간이 초과되거나 오류가 반환될 때 프로듀서가 재시도하지 않으면, 메시지가 토픽에 기록되지 않아 consumer 에게 전달되지 않을 수 있음
    2. 중복 가능성을 피하기 위해 때때로 메시지가 전달되지 않을 수 있음을 허용
2. At-Least-Once (최소 한번) : 최소 1번 전달되는것은 보장이 되지만, 중복 발생 가능(Acks -1)
    1. 프로듀서가 브로커로부터 ack (레코드 전송에 대한 응답)을 수신하고, acks 가 -1 인 경우 Topic 에 최소 한 번 작성되었음을 의미
    2. 그러나 ack 가 시간 초과 되거나 오류를 수신하면 메시지가 토픽으로 전송되지 않았다고 가정하고 프로듀서는 중복된 메시지를 전송할 수 있음
    3. 브로커가 ack 를 보내기 직전에 실패했지만, 실제로는 토픽에 기록된 후에 재시도를 수행하면 메시지가 두 번 기록될 수 있음
3. Exactly-Once : 정확히 한번)
    1. 프로듀서가 메시지 전송을 다시 시도하더라도 메시지가 컨슈머에게 정확히 한번만 전달됨
    2. 메시징 시스템 자체와 메시지를 생성하고 소비하는 애플리케이션 간의 협력이 반드시 필요
    3. 예를 들어, 메시지를 성공적으로 사용한 후 컨슈머를 이전 offset 으로 되감으면 프로듀서는 정확히 1번만 전달했지만 컨슈머가 2번 처리하게 될 수도 있음

---

### EOS (Exactly-Once-Semantic)

**************************************Idempotent Producer : 해당 옵션을 true 로 설정해야 한다.**************************************

- 프로듀서가 재시도 하더라도 메시지 중복을 방지한다.
- 성능에 영향이 별로 없다.

****Transactional****

- 각 프로듀서에 고유한 트랜잭션 ID 를 설정
- 프로듀서를 Transaction API 를 사용하여 개발
- Consumer 에서 isolation.level 을 read_committed 로 설정

프로듀서가 ********************메시지를 전송할 때 과정********************

1. 메시지는 PID (프로듀서 ID) 와 Sequence Number 를 가지고 있음. 이 정보들을 포함해서 브로커에 전송

<img width="694" alt="스크린샷 2023-09-10 오후 9 18 00" src="https://github.com/ROMANIAPEOPLE/apache-kafka/assets/39195377/9d95d374-58b8-4c3e-b8dd-41033110d2d6">


2. 브로커에 아래와 같이 저장되고, 메모리에 map 형태로 ( producer Id 를 key 로, 시퀀스 number 를 value 로) 저장되고, 이 map 은 *.snapshot 파일로 저장된다.

<img width="668" alt="스크린샷 2023-09-10 오후 9 18 17" src="https://github.com/ROMANIAPEOPLE/apache-kafka/assets/39195377/16685e4b-305d-4d28-9cb1-28f4c904e495">


3. 만약 producer 가 ack 를 받지 못해서 동일한 메시지에 대한 재시도가 이루어 지게 되면, 브로커는 프로듀서에게 ack - duplicate (DUP) 를 리턴한다.

### 트랜잭션 관련

https://yeongchan1228.tistory.com/62
