# 아파치 카프카

### 기본 용어 정리

- 주키퍼(Zookeeper) : 아파치 프로젝트 애플리케이션으로 카프카의 메타데이터(metadata) 관리 및 브로커의 정상상태 점검(health check) 을 담당 합니다.
    - 토픽 생성, 제거 및 브로커의 추가와 제거 등  변경사항에 대해 카프카에 알림.
    - 주키퍼 서버의 클러스터를 Ensmble 이라고 하는데, 이 엔서블이 몇 대로 구성되어있는지에 따라 주키퍼의 동작 여부가 결정된다.
    - 만약 Ensemble 이 5대로 구성되어 있고 2대가 장애가 발생했다면 → 정상 동작
        
        → 즉, 과반 이상이 정상 상태여야 정상 동작한다.
        
    - 따라서 주키퍼는 홀수 개의 서버로 작동하게 설계되어 있다(최소3,권장5)
- 카프카(Kafka) or 카프카 클러스터(Kafka cluster) : 아파치 프로젝트 애플리케이션으로 여러 대의 브로커를 구성한 클러스터를 의미 합니다.
- 브로커(broker) : 카프카 애플리케이션이 설치된 서버 또는 노드를 의미 합니다.
- 프로듀서(producer) : 카프카로 메시지를 보내는 역할을 하는 클라이언트로 총칭합니다.
- 컨슈머(consumer) : 카프카에서 메시지를 꺼내가는 역할을 하는 클라이언트를 총칭합니다.
- 토픽(topic) : 카프카는 메시지 피드들을 토픽으로 구분하고, 각 토픽의 이름은 카프카 내에서 고유합니다.
- 파티션(partition) : 병렬 처리 및 고성능을 얻기 위해 하나의 토픽을 여러 개로 나눈 것을 의미합니다
- 세그먼트(segment) : 프로듀서가 전송한 실제 메시지가 중개인의 로컬 디스크에 저장되는 파일을 말합니다.
- 메시지(message) 또는 레코드(record): 프로듀서가 브로커로 전송하거나 컨슈머가 읽어가는 데이터 조각을 말합니다.

---

- 토픽은 RDBMS 의 테이블과 같은 기능이다.
- 토픽은 여러 파티션을 가지는데 (하나만 가질수도있음), 프로듀서에서 데이터를 보내면 모든 파티션에 데이터가 저장되는것이 아니라, **여러 파티션 중 하나에만 데이터가 적재된다.**
- 토픽은 한 번 생성하면 이름 변경이 불가능하다.
- 파티션의 구조는 큐(FIFO) 구조이다. 즉, 컨슈머는 먼저 들어온 데이터를 먼저 꺼내간다.

---

### 빅데이터 파이프라인에 적합한 카프카의 특징

1. 높은 처리량 : 카프카는 프로듀서가 브로커로 데이터를 보낼때와, 브로커로부터 컨슈머가 데이터를 가져갈때 모두 묶어서 전송하는데, 그 이유는 많은 양의 데이터를 송수신할 때 맺어지는 네트워크 비용이 크기 때문이다.
그리고, 파티션 단위를 통해 동일 목적의 데이터를 여러 파티션에 분배하고 데이터를 병렬 처리할 수 있다. 즉, 파티션의 개수와 컨슈머의 개수를 동일하게 해서 데이터 처리량을 늘리는 것이다.
( 파티션1은 컨슈머 1을, 파티션2는 컨슈머 2를)
2. 확장성 : 처리량에 따라 브로커를 스케일아웃 하거나, 스케일 인 할 수 있다. 이 작업은 무중단 운영을 지원하기 때문에 365일 24시간 데이터를 처리해야하는 곳에서 유용하게 쓰인다.
3. 영속성 : 카프카는 다른 메시징 큐와 다르게 파일 시스템에 저장하기 때문에 컨슈머가 소비한다고 데이터가 삭제되지 않는다. 또한, 하나의 브로커가 망가지더라도 파일시스템에 저장되어 있기 때문에 영속이 유지된다.
→ 파일시스템에 저장하면 느리다고 생각할수도 있지만, 운영체제를 최대한 효율적으로 사용하는데, 페이지 캐시 메모리 영역을 사용하게 된다. (한 번 읽은 데이터는 메모리에 저장시켰다가 다시 사용하는 방식)
4. 고가용성 :  3개 이상의 서버들로 운영되는 카프카 클러스터(여러개의 브로커)는 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다. 하나의 브로커가 박살나면 다른 브로커에 데이터 복제가 되어 있어서 고가용성을 유지할 수 있다. 즉, 프로듀서가 데이터를 보내면 여러 브로커 중 하나의 브로커에만 데이터를 저장하는것이 아니라 다른 브로커에도 저장이 된다.

**************************************************************************파일시스템에 어떻게 저장될까?**************************************************************************

1. 프로듀서가 레코드를 send()
2. 브로커로 넘어감
3. 브로커에서 OS Page Cache 로 데이터를 쓰고
4. flush 가 일어나면 그 때 Disk에 저장됨

**페이지 캐시**는 처리한 데이터를 메인 메모리 영역(RAM)에 저장해서 가지고 있다가, 다시 이 데이터에 대한 접근이 발생하면 disk에서 IO 처리를 하지 않고 메인 메모리 영역의 데이터를 반환하여 처리할 수 있도록 하는 컴포넌트다.

→ Kafka 의 레코드는 네트워크 버퍼에 있는 데이터를 곧장 Page Cache 로 전송해 버린다. 그렇기 때문에 CPU 를 많이 필요로 하지 않고, Broker Heap 메모리를 절약할 수 있다.

→ 따라서 producer 가 broker 로 데이터를 전달하면, 그 데이터는 OS page cache 에 저장되어 있고, consumer 는 os page cache 에서 데이터를 가져가게 되는 것이다.

 ********************flush 되기 전에 장애가 나서 Disk 까지 가지 못하면?********************

→ 그래서 replication 을 사용하는 것.

---

### 브로커, 클러스터, 주키퍼

- 한 개의 카프카 클러스터에는 여러개의 브로커가 존재하며, 일반적으로 3개로 운영한다.
- 브로커는 카프카의 프로세스라고 생각하면 된다.
- 브로커는 카프카 클라이언트와 데이터를 주고받기 위해 사용하는 주체이자, 데이터를 분산 저장하여 장애가 발생하더라도 안전하게 사용할 수 있도록 도와주는 애플리케이션이다.
- 서버 1개 = 브로커 1개로 실행한다.
- 주키퍼는 카프카 클러스터를 운영하기 위해서 반드시 필요하다. (카프카 2 ver 에서는 필수, 3 ver 에서는 필수가 아니지만, 아직 완벽하지 않기 때문에 대부분 주키퍼를 끼고 운영한다.)
- 브로커를 여러 개 운영하는 이유는, 카프카 프로듀서가 데이터를 pub 했을 때 한 개의 브로커에만 저장되는 경우 브로커에 문제가 생겼을 때 데이터가 유실되기 때문이다.
    
    ![스크린샷 2023-05-30 오전 8.45.00.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/da49e27f-94bf-432c-a4ad-998cc769a057/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-30_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_8.45.00.png)
    
- 상황에 따라서는 주키퍼 앙상블 (N개 이상의 주키퍼를 포함)이 여러개의 카프카 클러스터(N개 이상의 브로커를 포함)를 관리하는 형태가 될 수 있다.
- 카프카 3.0 부터는 주키퍼가 없어도 클러스터가 동작 가능하다. 단, 아직은 미완성

### 브로커

하나의 카프카 클러스터 안에 여러개의 브로커가 존재할 수 있다. 
→ 브로커 A에 파티션 0,1,2 가 있고, 브로커 B에 파티션 0,1,2 가 있으면 이 때 리더 파티션은 브로커A에서는 파티션 0이, 브로커 B 에서는 파티션1이 담당하여 하나의 브로커로 부하가 집중되는 것을 방지할 수 있다.

브로커의 역할은 기본적으로 컨트롤러와 데이터 삭제이다.

1. **컨트롤러** : 클러스터의 다수 브로커 중 한 대가 컨트롤러의 역할을 하며 지속적으로 상태를 체크한다.만약 브로커가 클러스터에서 빠지는 경우 해당 브로커에 존재하는 **리더 파티션을 재분배한다.** 

카프카의 목적은 지속적으로 빠르게 데이터를 처리하는 것 이므로 비정상인 브로커를 빠르게 클러스터에서 빼내는 것이 중요하다. (만약 컨트롤러의 역할을 하는 브로커에 이상이 생기면 다른 브로커가 컨트롤러 역할을 하게 된다.)

**2. 데이터 삭제** 

 ****카프카는 다른 MQ와 다르게 컨슈머가 데이터를 가져가도 삭제되지 않으며, 프로듀서나 컨슈머가 데이터 삭제를 요청할 수도 없다. 카프카의 데이터를 삭제할 수 있는 것은 오직 브로커 뿐이다. 데이터 삭제는 파일 단위로 이루어지는데, 이 단위를 ‘로그 세그먼트’ 라고 부르며, 이 로그 세그먼트 안에는 다수의 데이터가 들어깄기 때문에 일반적인 DB 처럼 특정 데이터만 콕 찝어서 삭제할수도 없다.

**3. 컨슈머 오프셋 저장 :** 컨슈머 그룹은 토픽이 특정 파티션으로부터 데이터를 가져가서 처리하고, 이 파티션의 어느 레코드 까지 가져갔는지 확인하기 위해 오프셋을 커밋한다. 커밋한 오프셋은 __consumer_offsets 이라는 토픽에 저장되는데 이것은 인터널 토픽으로 사용자가 직접 확인할 일은 매우 드물다. 여기에 저장된 오프셋을 보고 컨슈머 그룹은 다음 레코드(데이터) 를 가져가는 것이다.

4. ******************************************그룹 코디네이터 :******************************************  코디네이터는 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배하는 역할을 한다. 예를들어 컨슈머 그룹 내부에 있는 컨슈머가 문제가 생겨 동작하지 않게 되면 해당 컨슈머에게 데이터를 주던 파티션을 정상적인 컨슈머와 연결시켜 재할당 (리밸런싱)을 담당한다.

### 브로커의 역할 - **데이터 저장과 삭제**

 ****카프카는 파일시스템에 데이터를 저장하는데,  config/server.properties 의 log.dir  옵션에 정의한 디렉토리에 데이터를 저장한다.
예를들어, hello.kafka 라는 디렉토리에 데이터가 저장되는데,  hello.kafka-0 부터 시작해서 최대 세그먼트 크기 만큼 (디폴트는 1기가) 용량이 차게 되면 hello.kafka-1 …2…3 이런식으로 늘어난다. 현재 디렉토리를 액티브 세그먼트 파일 이라고 하는데, 최근 데이터는 바로 이 곳에 저장된다.  (hello.kafka1,2,3 이 있으면 3이 엑티브임)

******************************************cleanup.policy=delete / compact******************************************
→ 액티브 세그먼트에 저장되어있는 파일(레코드) 들은 브로커의 삭제 대상에 포함되지 않는다.
→ 액티브 세그먼트 외 세그먼트도 데이터를 가져간다고 해서 삭제되는것은 아니고, 설정된 retention 옵션에 따라 용량이나 시간이 지남에 따라 세그먼트 단위로 삭제된다.  

![스크린샷 2023-05-31 오후 9.37.28.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/82916f84-f803-4739-8de1-0abd979c2a38/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-31_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_9.37.28.png)

→ 리텐션은 적절히 설정해야 한다. 파일시스템 자체가 브로커의 디스크에 저장되기 때문에 무한한 용량이 아니다. 만약 디스크의 용량이 5테라인데, 하루에 1테라가 들어오는 서비스를 운영한다면 7일로 설정되어 있을 경우 용량이 부족하게 된다. 기본적으로 **3일로 유지 한다.**

→ compact 로 설정을 하게 되면, 세그먼트 단위로 동일한 KEY 가 있을 때, 오래된 KEY에 해당하는 레코드를 제거한다. 압축이 완료된 영역을 **********************테일 영역********************** 이라고 하고, 압축이 되기 전 영역을 ********************헤드 영역******************** 이라고 부른다.

### 브로커의 역할 - 복제

![스크린샷 2023-05-31 오후 10.28.31.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b5aff25e-1bee-4871-ae1c-4e227b40adf3/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-31_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_10.28.31.png)

- 카프카 클러스터 내에는 여러개의 브로커들이 존재하고, 브로커 내부에는 토픽이, 또 그 토픽은 여러개의 파티션으로 이루어져 있다.
- 복제는 파티션 단위로 진행된다. 복제를 하는 이유는 ********************고가용성********************  때문이다.
- 리더파티션으로부터 produce 와 consumer 가 일어나고, 팔로워 파티션은 이를 가져가서 자신의 파티션에 복제하는 역할을 한다.
- 만약 1GB의 데이터를 produce 한다면, 카프카에 저장된 용량은 1GB가 아니고, replica factor 만큼 곱한 값이 된다.
- 브로커가 다운되면 내부에 있는 리더 파티션도 다운되는데 팔로워 파티션이 리더를 물려받는다.
- **********************************통상적으로 카프카 운영에는 replica factor 를 2 또는 3으로 구성한다. (데이터가 유실되어도 무관하거나, 데이터 처리가 빠르게 진행되어야 한다면 1로 설정한다. (예를들어, 1분당 60번 이상 정보를 보내는 GPS 데이터의 경우 2-3개 정도는 유실되어도 무관하기 때문에 1로 설정하곤 한다. 반대로 금융정보와 같은 유실되면 절대로 안되는 경우는 3으로 설정하는 경우도 있다.)**********************************
- 리더 파티션에 있는 레코드들을 팔로워 파티션이 복제하는 행위를 Fetch 라고 한다.

### ISR (In-Sync-Replicas)

ISR 이란 리더파티션과 팔로워 파티션의 오프셋이 동일한 상태 (= 즉, 모두 싱크가 맞는 상태를 뜻한다)

프로듀서가 데이터를 pub 하게 되면 리더 파티션에 데이터가 쌓이게 되고, 팔로워 파티션(복제)는 리더 파티션의 offset 과 자신의 offset 을 비교해서 두 값이 다르다면 복제를 시작하게 되는 구조이다.

그런데 만약, 두 파티션(리더와 팔로워)가 offset을 비교하고 복제를 시작하기 전에 리더 파티션의 브로커가 고장이 난다면 ?

팔로워 파티션 중 하나가 리더 파티션으로 승격될 것이다. **이 때 미처 복제하지 못한 데이터는 유실되고 만다.**

```java
// 유실을 감수한다. 복제가 안된 팔로워 파티션을 리더로 승급시킨다. 
// 즉, OSR 상태의 파티션도 리더로 승급시킬 수 있다.
unclean.leader.election.enable=true 

// 유실을 감수하지 않는다. 해당 브로커가 복구될 때까지 중단한다. (주로 금융데이터)
unclean.leader.election.enable=false
```

**************************************************false 로 설정한 상태에서 ISR 상태의 partition 이 없다면?**************************************************

→ 해당 브로커에 대한 프로듀서의 쓰기와 컨슈머의 읽기가 모두 중단된다. 이렇게 되면 중단 되는 동안 카프카로 데이터가 전달되지 않아 데이터 유실이 발생할 수 있다.

- 의도적인 유실
- DR 클러스터 구성 (dead-letter-queue)

**********ISR 또는 OSR 을 판단하는 2가지 옵션**********

- replica.lag.max.messages : 메시지의 개수로 ISR 또는 OSR 여부를 판단하는 옵션이다.
    - 메시지가 항상 일정한 비율로 kafka 에 들어올 때, 해당 옵션을 5로 설정하면, 5개 이상 지연되는 경우 OSR 로 판단하고, 그 이하는 ISR 로 판단한다.
    - BUT, 메시지의 유입량이 갑자기 늘어나는 경우 일시적으로 복제가 늦어지는 것 뿐인데 5개 이상 지연으로 판단하고 OSR 로 판단하게 된다
    → 팔로워는 정상적으로 동작하고 단지 잠깐의 지연이 발생했을 뿐인데..
- [replica.lag.time.max.ms](http://replica.lag.time.max.ms) : 해당 옵션으로 판단해야 한다. (팔로워가 리더로 Fetch 요청을 하는데 걸리는 시간)
    - 팔로워가 리더로 Fetch 요청을 보내는 Interval 을 체크한다.
    - 예를들어, 해당 옵션이 10000 이라면 팔로워가 리더로 Fetch 요청을 10000ms 내에만 보내면 정상으로 판단
    - Confluent 에서는 해당 옵션만 남겨놓고, message 옵션은 제거해버림.

****High Water Mark : ISR 간에 복제된 Offset 으로, Committed 라고도 부른다.
→ 새로운 레코드가 들어와서 팔로워 파티션에 모두 복제가 완료되었다면 하이워터마크를 하나 올린다.
→ 컨슈머는 Committed 메시지만 읽을 수 있다. (즉, high watermark 까지만 읽을 수 있음)****

![스크린샷 2023-09-04 오후 11.11.03.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/f1c667e5-ce5d-488c-bf7e-d906298dff57/0878416d-7128-43b6-b067-618a301be172/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-09-04_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_11.11.03.png)

************************************참고로, ISR 또는 OSR 의 여부는 리더 파티션이 속해있는 브로커가 판단한다.************************************

→ ISR 상태는 각 파티션의 리더가 관리한다. 만약 특정 팔로워 파티션이 너무 느려서 ISR → OSR 이 되었다면 해당 파티션의 리더는 주키퍼로 변경된 정보를 전달한다.

그리고 주키퍼는 다시 컨트롤러로 신규 ISR 팔로워 정보를 전달하게 된다.

(주키퍼가 관리하는 카프카의 메타데이터 중 하나임)

********************************정리하자면 (LEO 와 high watermark 에 대해)********************************

- 결국 consumer 는 high watermark 까지의 레코드만 소비할 수 있다.
- LEO (Log End Offset) 은 그 프로듀서가 보낸 데이터가 적재될 offset 을 가르킨다.
- 즉, 안전하게 사용하기 위해 (고가용성) ISR 에 성공적으로 복제된 지점을 나타내는 high watermark 의 레코드 까지만 소비를 하는 것이다.
- high watermark 는 컨슈머 그룹에서 소비한 가장 최신의 offset 이다.

**정리하자면 (fail over 에 대해)**

- Follower 가 실패하는 경우, Leader 에 의해 ISR 리스트에서 삭제되고, 리더는 새로운 ISR을 사용하여 커밋함.

- Leader 가 실패하는 경우
    1. 주키퍼가 먼저 리더 파티션이 존재하는 브로커의 장애를 감지한다.
    2. 컨트롤러 (그룹 코디네이터) 는 새로운 리더와 ISR 상태의 팔로워를 선출한 후 주키퍼로 그 정보를 전달한다.
    3. 주키퍼는 모든 브로커들에게 그 정보를 전달한다.

---

 ****************************************************************

### 토픽 생성시 파티션 배치

![스크린샷 2023-06-01 오후 11.30.05.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cb52fbc6-4646-421e-ab7c-5b5376987a21/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-06-01_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_11.30.05.png)

- 위 그림처럼 파티션이 5개인 토픽을 생성했을 경우, 라운드 로빈 방식으로 하나씩 각 브로커에 파티션이 배치된다.

![스크린샷 2023-09-05 오전 11.42.17.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/f1c667e5-ce5d-488c-bf7e-d906298dff57/c2ff0660-fc30-47bf-9392-6a5f9b037ee2/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-09-05_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_11.42.17.png)

![스크린샷 2023-06-01 오후 11.31.40.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6ea596df-10b5-4086-bae8-98dc65d9531c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-06-01_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_11.31.40.png)

- 브로커마다 리더 파티션은 파티션0부터 시작하게 된다. 
즉, 브로커0 에는 리더파티션이0, 브로커1에는 리더 파티션이1, 브로커 2에는 리더 파티션이 2, 다시 브로커 0으로 돌아가서 리더 파티션은 3이 된다.
간혹, 특정 브로커에 리더 파티션이 쏠리는 현상이 있는데, 바로 이것이 카프카 클러스터를 운영할 때 중요한 부분이다.
- `[kafka-reassign-partitions.sh](http://kafka-reassign-partitions.sh)` 라는 명령어를 사용하여 파티션을 재배치할 수 있다.

### 파티션 개수와 컨슈머 개수

- 하나의 파티션은 하나의 컨슈머에만 데이터를 줄 수 있다. 즉, 파티션 하나가 여러개의 컨슈머에 데이터를 주는것은 불가능하다.
- 단, 하나의 컨슈머가 여러개의 파티션의 데이터를 가져가는것은 가능하다.
- 컨슈머 개수를 늘리면 파티션 개수도 늘려야 효과가 극대화 된다.
- consumer lag란, 프로듀서가 보내는 데이터의 양을 컨슈머가 따라가지 못하는 현상을 말한다.
- 파티션을 늘리는것은 자유롭게 가능하지만, **줄이는것은 불가능하다.** 만약 지원한다면? 파일시스템을 재분배 하는 과정을 거쳐야 하기 때문에 엄청난 리소스가 든다. (앞으로도 지원하지 않을듯?)

### 레코드

레코드는 타임스탬프, 헤더, 메시지 키, 메시지 벨류, 오프셋으로 구성되어 있다. 참고로 브로커에 한 번 적재된 레코드는 삭제할 수 없고, 리텐션 기간 또는 용량에 따라서만 삭제된다.

1. 타임스탬프 : 기본적으로  ProducerRecord의 createTime 이 들어가지만 (레코드가 생성된 시간) 설정을 변경하여 브로커에 레코드가 적재된 시간인 LogAppendTime 으로 설정할수도 있다.
(message.timestamp.type) 을 사용한다.
2. 오프셋 : 오프셋은 프로듀서가 데이터를 생성한 직후에는 존재하지 않으며, 프로듀서가 전송한 레코드가 브로커에 적재될 때 생성되는 것이다. 오프셋은 0부터 1씩 증가하게 된다. 각 메시지는 파티션별로 고유한 오프셋을 가지므로, 컨슈머에서 중복 처리를 방지하기 위한 목적으로도 사용한다.
3. 헤더 : key/value 데이터를 추가할 수 있으며 레코드의 스키마 버전이나 포캣과 같이 데이터 처리에 참고할만한 정보를 담아서 사용할 수 있다. (0.11 버전부터 지원)
4. KEY : 카프카는 프로듀서가 토픽으로 메시지를 전달할 때 토픽 내 어느 파티션에 메시지가 저장되는지 지정할 수 있는데, 바로 KEY를 사용해서 특정 파티션에 저장하는 것이다. KEY 값은 필수값이 아니며 지정하지 않을 경우 null 로 설정되며 라운드 로빈 방식으로 각 파티션으로 메시지가 분배된다. null 이 아닌 메시지 키는 해쉬값에 의해서 특정 파티션에 매핑되어 전달된다.
5. VALUE : **실질적으로 처리할 데이터로,** 제네릭형태로 사용자의 지정 타입으로 메시지가 전달된다. 컨슈머 입장에서는 어떤 타입으로 메시지가 토픽에 저장됐는지 모르기 때문에, 컨슈머 로직에 반드시 미리 역직렬화 포맷을 지정해야 한다.

### 카프카 클라이언트와 메타데이터

카프카 클라이언트는 프로듀서가 될 수도 있고, 컨슈머가 될 수도 있다. 프로듀서나 컨슈머 모두 특정 브로커에 존재하는 리더 파티션과 통신을 해야 데이터를 전송하거나 소비할 수 있는데, 이 메타데이터에는 리더 파티션의 위치가 들어있다. 따라서 메타데이터를 전송 받아 리더 파티션의 위치를 파악하고 불필요하게 팔로워 파티션과의 통신을 방지할 수 있다.

```java
metadata.max.age.ms : 메타데이터를 강제로 리프래시하는 간격으로, default는 5분
metadata.max.idle.ms : 프로듀서가 유휴상태일 경우 메타데이터를 캐시에 유지하는 기간. 
예를들어, 프로듀서가 특정 토픽으로 데이터를 보내고 지정한 시간이 지나면 데이터를 리프래시.
```

### 카프카 운영 방법

- 온프레미스 : 물리장비(컴퓨터/서버) 구매 후 네트워크 설치, 구성 → 카프카 설치 + 운영
→ 현실적으로 불가능. 모든것을 0부터 갖추어야 함.
- Iaas : AWS, GCP와 같은 클라우드 서비스를 통해 가상 컴퓨팅(ex.EC2) 리소스를 발급하여 오픈소스 카프카 설치 + 운영
→ 가장 흔한 방식
- Saas : AWS 의 MSK 와 같이 모든것을 구축해놓은 상품.
→ 다양한 주변 생태계 (모니터링 도구, ksqlDB) 를 옵션으로 제공, 직접 운영해본 경험이 부족한 경우 Saas로 노하우를 키운 후 Iaas 로 가는것도 좋은 방법.
→ 기본적인 보안도 적용되어 있고, 모니터링도 지원하며, 여러 브로커를 직접 모니터링 해야 하는 리소스를 방지해준다.
→ 단, Saas 로 사용하게 되면 해당 클라우드에 대한 종속성이 생기고 비용이 증가하게 된다.

### CLI (카프카 커맨트 라인 툴)

카프카에서 제공하는 카프카 커맨드 라인 툴들은 카프카를 운영할 때 가장 많이 접하는 도구다.

### 프로듀서

카프카에서 데이터의 시작점은 프로듀서이다. 프로듀서 애플리케이션은 카프카에 필요한 데이터를 선언하고 브로커의 특정 토픽의 파티션에 전송한다. 프로듀서는 데이터를 전송할 때 **리더 파티션을 가지고 있는 브로커가 직접 통신한다.**

**************************************************************************************************************************************************************************************************************************************************프로듀서는 카프카 브로커로 데이터를 전송할 때, 내부적으로 파티셔너, 배치 생성 단계를 거친다.**************************************************************************************************************************************************************************************************************************************************

🤖 참고로, 아파치 카프카가 공식적으로 지원하는 라이브러리는 자바 라이브러리 뿐이다. 그렇기 때문에 프로듀서를 자바가 아닌 다른 언어로 개발을 한다면 조금은 미흡한 라이브러리를 사용하게 될 가능성이 있다.

************************************************프로듀서의 내부 구조************************************************

![스크린샷 2023-06-04 오후 7.57.06.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d006a036-7b6e-4ab7-a1da-67a1b9a4acfc/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-06-04_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_7.57.06.png)

- ProducerRecord : 프로듀서에서 생성하는 레코드. 참고로 오프셋은 미포함 된다. (파티션에 들어간 이후 생김)
- send() : 레코드를 브로커로 전송하는 메소드
- Partitioner : 어느 파티션으로 전송할지 지정하는 파티셔너. 메시지 키가 존재한다면 해당 메시지 키를 해시값으로 변경한 후 특정 파티션에 지정한다. 기본값은 DefaultPartitoner
→ 만약 KEY 가 존재한다면, 특정 알고리즘을 이용해 KEY 를 HASH 화 한 후 partition 의 갯수 만큼 나머지 나눗셈을 통해 해당 레코드가 적재될 파티션의 위치를 선정하게 된다.
- Accumlator : 프로듀서에서 브로커로(카프카 클러스터) 로 데이터를 전송하게 되면 TCP 통신을 해야하는데, 매번 TCP 통신을 위한 3-way-handshake 와 4-way-handshake 의 부담을 덜어주기 위해 Batch 로 한 번에 데이터를 묶어서 전송할 때 사용한다. (높은 데이터 처리량)
→ 옵션에 따라서, 특정 시점에 sender 가 브로커(카프카 클러스터) 에 보낸다.
→ 스티키 파티셔너를 사용할 경우 어큐뮤레이터의 버퍼를 채워서 보내기 때문에 성능 향상에서 유리하다.

**********************************프로듀서의 메시지 전송 방법**********************************

- 사용자 코드 레벨에서 실행하는 Send() 메소드. 해당 메소드를 실행하게 되면 Accumulator 로 ProducerRecord 를 보낸다.
- 어큐뮬레이터에는 record 가 batch 단위로 쌓이게 되는데, 이 쌓인 r**ecord 들** (record batch 라고도 한다.)을 kafka broker 에 보내는것이 바로 Sender() 메소드 (=네트워크 스레드) 이다.
- record batch 의 크기는 `batch.size` 로 설정이 가능하다.
- 만약 Sender() 메소드가 카프카 브로커로 record batch 들을 전송하는 속도가 어큐뮬레이터에 데이터가 쌓이는 속도보다 느리다면 ?
→ 레코드 배치는 계속해서 추가되고, 레코드 배치의 크기가 `buffer.memory` 보다 커지면 send() 메소드 (사용자 코드 레벨에서 프로듀서 레코드를 어큐뮬레이터로 전송하는 기능)는 block이 된다.
- 만약 block이 되는 시간인 [**max.block.ms](http://max.block.ms) 를 초과하게 되면, exception 이 발생한다.**
- **즉, 빠르게 가져가야 한다.**
    
    ![스크린샷 2023-09-05 오후 9.52.37.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/f1c667e5-ce5d-488c-bf7e-d906298dff57/8cb56992-76f1-41cd-9531-614f81245c4c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-09-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_9.52.37.png)
    

********************************************프로듀서 - 파티셔너********************************************

UniformStickeyPartitioner 와 RoundRobinPartitioner 가 존재하는데, 카프카 클라이언트 라이브러리 버전에 따라 default Partitioner 가 다르다. (2.5.0 버전에서는 유니폼스티키파티셔너가 Default)

1. Message Key 값이 존재하는 경우
메시지 키가 존재하는 경우 기본 파티셔너 설정과 무관하게 KEY를 해쉬값으로 변환한 후, 특정 파티션에 데이터를 보낸다. 이후 동일한 KEY는 동일한 파티션으로 보내는 것이다.
**만약, 파티션 개수가 변경될 경우 메시지 키와 파티션 번호의 매칭이 깨지게 된다. (그래서.. 파티션 개수를 애초에 넉넉하게 설정해라. 그래야 추가할 일이 없다.)**
    1. BUT, 동일한 파티션에 레코드를 보내도, 파티션이 여러 개인 경우 전체(모든) 데이터에 대한 순서는 보장할 수 없다.
    2. 예를 들어, 총 3개의 파티션이 존재한다고 했을 때, 각각 파티션에 특정 데이터를 보낸다고 하면 특정 컨슈머가 바라보고 있는 각각의 파티션 마다 처리 속도가 다르기 때문에 순서가 보장되지 않는다.
    3. 파티션1 에서 2개를 처리할 동안 파티션 2개에 1개만 처리가 될 수 도 있기 때문이다.
2. Message Key 값이 존재하지 않는 경우
    1. Rount Robin : ProducerRecord 가 들어오는 대로 파티션을 순회하며 전송하고, 어큐뮤레이터에서 묶이는 정도가 적기 때문에 전송 성능이 낮음.
    → 쉽게 말해서, 어큐뮬레이터에 데이터가 적절히 쌓이기도 전에 라운드 로빈을 통해 각 파티션에 데이터를 적재하기 때문에 통신에 있어서 효율이 떨어진다는 의미.
    2. Uniform Stickey : ProducerRecord 를 어큐뮬레이터의 버퍼가 채워질 때 까지 기다렸다가 보낸다. 따라서 네트워크 통신에 필요한 오버헤드를 최소화 할 수 있으며, record를 고르게 보내지 말고 이번에 보낼 때는 이쪽, 다음에는 저쪽으로 보내서 record batch를 꽉 채워서 보내자는 게 주 내용이다.

******************************************************************************************프로듀서 주요 옵션(필수 옵션 = default 가 없기에 반드시 설정 필요)****************************************************************************************** 

- bootstrap.servers : 카프카 클러스터에 속한 브로커의 호스트이름:포트를 1개 이상 작성한다. 2개 이상 브로커 정보를 입력하여 일부 브로커에 이슈가 발생하더라도 접속하는 데에 이슈가 없도록 설정 가능하다.
- key.serializer : 레코드의 메시지 키를 직렬화하는 클래스를 지정한다.
- value.serializer : 레코드의 메시지 값을 직렬화 하는 클래스를 지정한다. 
→ String serializer 로 직렬화를 하면 `kafka-console-consumer` 에서 디버깅이 가능한데, Int 나 float 과 같은 형태를 String serializer 로 묶어서 직렬화 하게 되면 producer record 를 broker 로 보낼 때 네트워크 비용이 더 많이 들어간다.
→ 참고로 역직렬화는 코드레벨에서 직접 작성해야 한다. (여기서 String serializer 로 통일하게 되면 타 팀과의 작업에서 역직렬화로 인한 낭패를 겪을 일은 없다)
→ 이에 따른 손익은 알아서 판단하자.

************************************************프로듀서의 배치 처리************************************************

1. [linger.ms](http://linger.ms) (default : 0, 즉시 보냄) / 일반적인 설정은 100
    
    메시지가 함께 Batch 처리 될 때까지 대기 시간
    
2. batch.size (default : 16KB) / 일반적인 설정은 100만
    
    보내기 전 batch 의 최대 크기
    

→ 상황에 따라 다르지만 대부분 1번을 사용함. (이유는 batch.size 로 설정 시 처리량이 적어 배치가 쌓이지 않아서 브로커로 전송되지 않는 케이스가 존재하기 때문)

→ 둘 다 함께 사용할 수 있고, [linger.ms](http://linger.ms) 를 설정해도 batch.size 로 설정한 크기 만큼의 레코드가 차면 linger.ms 값은 무시된다.

****************************************************************************프로듀서 주요 옵션(선택 옵션)****************************************************************************

- acks (default : 1) 프로듀서가 브로커로 데이터를 전송할 때 정상적으로 저장이 되었는지 전송 성공 여부를 확인하는데 사용하는 옵션
- [linger.ms](http://linger.ms)(default : 0) : producer record 가 어큐뮬레이터로 넘어가고, 이 어큐뮬레이터 안에 있는 레코드 배치들이 sender() 메소드를 통해 파티션으로 전송되는데 까지 얼마나 기다릴지 설정하는 것. 만약 record batch 에 레코드들을 좀 채워서 보내고 싶다면 숫자를 늘려야 한다. (단, 처리속도는 감소한다)
- retries (default: INTEGER.MAXVALUE()) : 브로커로부터 에러를 받고 난 뒤 재전송을 시도하는 횟수 (재시도 할 생각 없으면 0 또는 낮게 설정)
- max.in.flight.requests.per.connection(default : 5) : 한 번에 요청하는 최대 커넥션 개수. 즉, Sender 메소드 (=network thread) 의 스레드 개수 설정.
- partitioner.class :  파티셔너 설정. defulat 는 2.5.0 부터는 유니폼 스티키, 전은 라운드 로빈

************acks 옵션************

카프카 프로듀서의 acks 옵션은 0,1,-1(또는 all) 값을 가질 수 있다.

이 옵션을 통해 프로듀서가 전송한 데이터가 카프카 플러스터에 얼마나 신뢰성 높게 저장할지 지정할 수 있다. 이 옵션에 따라 성능이 달라질 수 있다. (단, replica factor 가 1인 경우는 성능 변화가 크지 않다.) 

그렇지만 실무에서는 대부분 replica factor 를 2 이상으로 두고 운영을 하기 때문에, acks 옵션에 따른 각각의 성능 차이를 잘 알아야 한다.

- acks 0 : 프로듀서가 리더 파티션으로 데이터를 전송했을 때 무조건 전송이 완료 되었다고 생각하는 설정이다. 즉, 정상적으로 저장이 되었는지 확인하지 않는다. 기본적으로 프로듀서는 리더 파티션에 데이터가 저장이 되면 몇 번째 오프셋에 저장되었는지 리턴하는데 이것을 리턴 받지 않겠다는 뜻이다. 
→ 데이터가 일부 유실되더라도 상관 없는 경우 사용한다. (로그?)
- acks 1 : 프로듀서가 보낸 데이터가 리더 파티션에만 정상적으로 적재 되었는지 확인한다. 리더 파티션에 정상적으로 적재되지 않았다면, 리더 파티션에 적재될 때까지 재시도할 수 있다. 리더 파티션에 데이터가 저장되었다는 것은 파일시스템이 디스크에 저장되었다는 의미로, 이러한 응답을 받기 까지는 시간이 좀 걸린다. (0보다 오래 걸림)
→ 단, 리더 파티션만 확인하기 때문에 팔로워 파티션에는 아직 데이터가 동기화 되지 않았을수도 있기 때문에.. 리더 파티션의 브로커에 장애가 발생헀을때 아직 동기화되지 못한 팔로워 파티션이 리더로 승급할 경우, 일부 데이터가 유실될 수 있다. (단, 매우 드물기 때문에 대부분 실무에서는 1로 설정한다.)
    
    **추가 : acks 1 의 at most once**
    
    - 리더파티션 1개와 팔로워 파티션 2개가 있다고 가정
    - 리더파티션에는 (m1,m2,m3,m4) 의 레코드가 존재하고, hw 는 m3 이다.
    - 팔로워 파티션 중 하나가 ISR 상태 (즉, m1,m2,m3 까지 복제가 완료된 상태)
    - 리더 파티션에 장애가 발생하게 되면 ISR 팔로워 파티션이 리더가됨.
    - 팔로워에서 리더로 승격한 파티션에는 (m1,m2,m3) 의 레코드가 존재
    - acks 1의 옵션으로 인해 기존 리더파티션에 m4 까지 정상적으로 발행되었기 때문에 retry 하지 않음
    - m4 레코드 영구 유실.
- acks -1 (또는 all 이라고도 부른다) : 리더/팔로워 모두 정상적으로 적재 되었는지 확인한다. acks 를 all 로 설정한 경우 토픽 단위로 설정이 가능한 `min.insync.replicas` 옵션에 따라 데이터의 안정성이 달라진다. 이 설정은, 만약 replica factor 가 2인 카프카 클러스터의 경우, 총 3개의 브로커를 가지게 되는데, 이 설정을 2로 할 경우 리더1개와 팔로워 1개만 all(-1) 에 대한 설정을 적용받는다. 대부분 2로 설정하면 된다. 모든 브로커서버가 박살날 가능성은 로또 맞을 확률과 비슷하다. (하나의 리더와 하나의 팔로워만 확인 해도 신뢰도 높은 애플리케이션을 만들 수 있다는 뜻)
→ 신뢰도는 매우 높고, 처리 속도는 낮다. (데이터 처리량이 너무 낮아서 사용하지 못하는 수준임)
→ 절대 절대 유실되어서는 안되는 데이터 + 처리량이 매우 낮아도 아무런 상관 없는 경우 사용.
→ at least once (최소 한 번) 전송을 보장.
    
    

******************acks 옵션 중 0 을 제외한 1 또는 -1 일 때 Producer Retry****************** 

![스크린샷 2023-09-05 오후 8.46.47.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/f1c667e5-ce5d-488c-bf7e-d906298dff57/ea99d23e-6579-43f0-bd4f-3c0e0010bc4b/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-09-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_8.46.47.png)

→ 대부분 retries 를 조정하는 대신에 `[delivery.timeout.ms](http://delivery.timeout.ms)` 조정으로 재시도 동작을 제어한다.

![스크린샷 2023-09-05 오후 9.00.02.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/f1c667e5-ce5d-488c-bf7e-d906298dff57/722b457e-62c8-4f11-b7a4-59553b01caa3/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-09-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_9.00.02.png)

---

### 컨슈머

![스크린샷 2023-09-04 오후 10.47.04.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/f1c667e5-ce5d-488c-bf7e-d906298dff57/87db1157-41ba-4800-b59a-d98780e673ca/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-09-04_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_10.47.04.png)

********************************************컨슈머의 내부구조********************************************

![스크린샷 2023-06-05 오후 11.42.01.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5a2bb0bc-bd06-4221-8c25-5d5ea9a8c728/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-06-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_11.42.01.png)

- Fetcher : 리더 파티션으로부터 레코드들을 미리 가져와서 대기.
→리더 파티션에서 Fetcher 에 데이터를 가져올때 배치로 미리 가져오기 때문에, 데이터 처리 속도가 빨라도 상관이 없다.
- poll() : Fetcher 에 있는 레코드들을 리턴하는 메소드
- ConsumerRecords : 처리하고자 하는 레코드들의 모음. 오프셋을 포함하며, 복수형이다.

**************************컨슈머 그룹**************************

컨슈머 그룹으로 운영하는 방법은 컨슈머를 각 컨슈머 그룹으로부터 격리된 환경에서 안전하게 운영할 수 있도록 도와주는 카프카의 독특한 방식이다.

컨슈머 그룹으로 묶인 컨슈머가 토픽을 구독해서 데이터를 가져갈 때, 1개의 파티션은 최대 1개의 컨슈머에만 데이터를 줄 수 있다. 즉, 파티션A 가 Counsmer Group A에 속한 Consuemr-1 과 Consumer-2 중 1곳에만 데이터를 전송할 수 있다는 것이다.

단, 컨슈머는 여러개의 파티션에게 데이터를 받을 수 있다.

- 파티션 > 컨슈머 : 하나의 컨슈머가 2개의 파티션에게 데이터를 받는다.
- 파티션 < 컨슈머 : 노는 컨슈머 (유휴상태인 컨슈머) 가 생긴다.
→ 스레드만 차지하고 실질적인 데이터 처리를 하지 못하므로 애플리케이션 실행에 있어 불필요한 스레드로 남게 된다.

![스크린샷 2023-06-06 오전 12.21.13.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e6c55fe6-5c07-44a4-8d6e-1217dc439b8a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-06-06_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_12.21.13.png)

이렇게 리소스 수집 에이전트에서는 단순히 Topic 에 데이터를 producing 만 해주고, 여러 Consumer Group 을 구성해서 각각 알맞은 로직을 구현하여 애플리케이션간의 결합도를 낮출수있다.

******리밸런싱******

컨슈머 그룹으로 이루어진 컨슈머들 중 일부 컨슈머에 장애가 발생하며느 장애가 발생하던 컨슈머에게 데이터를 보내던 파티션은 재빨리 정상적인 컨슈머에게 데이터를 주도록 해야하는데, 이 과정을 리밸런싱 이라고 부른다. (참고로 컨슈머 리밸런싱 과정에서 컨슈머는 멈춘다.)

- 컨슈머가 추가될 때
- 컨슈머가 제외 (장애로 인해) 될 때
- 파티션이 추가될 때 (파티션은 추가만 가능하고 제거할수는 없음)
- 컨슈머가 Topic 구독을 변경할 때

리밸런싱은 위 상황에서 발생하며, 데이터 처리 중 발생한 리밸런싱에 대응하는 코드를 작성해야 한다. (리밸런싱 리스너가 존재한다.)

```yaml
// 리밸런싱 리스너 종류
onPartitionAssgined() : 리밸런스가 끝난 뒤에, 파티션 할당이 완료 되었을 때 호출됨.
onPartitionRevoked() : 리밸런스가 시작되기 직전에 호출하는 메소드로, 리밸런스가 시작 되기 직전에 처리된
record 를 기준으로 commit 을 해야 하기 때문에, 해당 메소드에 커밋을 구현하여 처리할 수 있다.
```

************Consumer Group 내에서 각 컨슈머들과 파티션을 할당하는 과정************

- broker 가 아닌 consumer Group 내 consumer Leader 를 선출하여 그 리더가 각 컨슈머들에게 파티션을 할당한다.
- 할당이 완료되면 leader 는 그룹 코디네이터(브로커, 또는 컨트롤러 라고도 함) 에게 할당 결과를 알린다.

→ 브로커가 직접 컨슈머를 할당하지 않는 이유는 간단하다. 수 많은 토픽와 컨슈머 그룹을 브로커가 모두 관리하기는 힘들기 때문이다.

******커밋******

컨슈머는 카프카 브로커 (토픽) 으로부터 데이터를 어디까지 읽었는지 offset 을 통해 알 수 있는데, 이는 commit 을 통해 기록한다.

특정 토픽의 파티션을 어떤 컨슈머 그룹이 몇 번째 offset 을 가져갔는지 카프카 브로커 내부에서 사용되는 내부 토픽 (__consumerOffsets) 에 기록된다. 만약 컨슈머 동작에 이슈가 생겨서 commit 이 발생하지 않아 내부 토픽( __consumerOffsets) 에 기록되지 않았다면, 데이터가 중복 처리될수도 있다.
→ 즉, 브로커에서 컨슈머가 데이터를 가져간 이후 commit 을 날려 오프셋을 기록해야 하는데, 어떠한 문제로 인해 데이터만 가져가고 커밋을 하지 못했다면 (또는 커밋을 했는데 내부 토픽에 오프셋을 기록하지 못했다면) 중복 데이터가 발생할 수 있다.

**어싸이너(Assignor)**

무엇을 써도 크게 상관은 없지만, 2.5.0 에서는 RangeAssignor 가 default 다.

즉, 파티션과 컨슈머를 어떻게 매칭 시키며 할당할것인지에 대한 정책이다.

![스크린샷 2023-06-06 오전 12.37.01.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/50b96aa1-7d99-4fa7-9784-ecf89ddeb2ca/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-06-06_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_12.37.01.png)

******************************************컨슈머 주요 옵션(필수 옵션)******************************************

- bootstrap.servers : 프로듀서가 데이터를 전송할 대상 카프카 클러스터에 속한 브로커의 호스트 이름:포트를 1개이상 작성한다. (마찬가지로 2개 이상 작성해야 일부 브로커에 이슈가 생겨도 접속 가능)
- key.deserializer : 레코드의 메시지 키를 역직렬화
- value.deserializer : 레코드의 메시지 값을 역직렬화

**************************************************************************컨슈머 주요 옵션 (선택 옵션)**************************************************************************

- [group.id](http://group.id) : 컨슈머 그룹 아이디를 지정한다. subscribe() 메서드로 토픽을 구독하여 사용할 때는 이 옵션을 필수로 넣어야 한다.
- auto.offset.reset : 컨슈머 그룹이 특정 파티션을 읽을 때, offset 이 없는 경우 어디부터 읽을지 선택하는 옵션이다. (한 번도 commit 되지 않은 파티션에는 offset이 없다.) 기본 옵션은 latest 이다.
    - latest : 가장 높은(가장 최근) 오프셋 부터 읽기 시작
    - earliest : 가장 낮은 (가장 오래전에 넣은) 오프셋 부터 읽기 시작
    - none : 커밋한 기록을 찾아보고, 기록이 없으면 오류 반환 (사용 거의 안 함)
    - ********************************************************************************************************************************************************************한 번이라도 커밋이 된 이력이 있다면 위 옵션은 싹 다 무시된다.********************************************************************************************************************************************************************
- enable.auto.commit : true/false 로 설정할 수 있으며, 특정 시간마다 자동으로 커밋을 해서 offset 을 지정하는 옵션이다. (default : true)
- [auto.commit.interval.ms](http://auto.commit.interval.ms) : enable.auto.commit 을 true로 설정한 경우 몇 초 마다 commit 을 할 것 인지 설정하는 옵션이다. (default : 5000ms = 5초)
- max.poll.records : poll() 메서드를 통해 반환되는 레코드의 개수를 지정하며, 기본값은 500이다. 만약 더 많은 데이터를 처리하고 싶다면 숫자를 늘려라.
- [session.timeout.ms](http://session.timeout.ms) : 컨슈머가 브로커와 연결이 끊기는 최대 시간이다. 기본값은 10000 (10초)
- [heartbeat.interval.ms](http://heartbeat.interval.ms) : 하트비트를 전송하는 시간 간격이다. 기본값은 3000 (3초)
→ 즉, 하트비트를 브로커에 3초 간격으로 계속해서 보내다가, 하트비트가 오지 않으면 10초동안 기다렸다가, 그래도 안오면 익셉션 발생 → 리밸런싱이 이루어짐
- [max.poll.interval.ms](http://max.poll.interval.ms) : poll() 메소드를 호출하는 간격의 최대 시간. 기본값은 5분이다.
→ 5분이 지났는대도 poll() 메소드를 호출하지 않는다면 리밸런싱 대상.
- isolation.level : 트랜잭션 프로듀서가 레코드를 트랜잭션 단위로 보낼 경우 사용 (잘 사용하지 않음)

********************수동커밋******************** 

대부분 자동 커밋을 하지만, 수동 커밋을 해야하는 경우도 있다.

1. 동기 오프셋 (레코드 단위) 커밋  : poll() 메서드가 호출된 이후, commitSync() 메서드를 호출하여 오프셋 커밋을 명시적으로 수행할 수 있다. commitSync() 메서드는 poll() 메소드의 리턴 결과인 comsumerRecords 의 마지막 record 처리가 끝난 이후에 진행 되어야 한다.
    
    ![스크린샷 2023-06-06 오후 3.36.41.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b2734b8c-b5f4-4be5-98c4-5ae32b252d96/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-06-06_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.36.41.png)
    
    → commit() 과정 자체가 컨슈머가 브로커와 통신을 해야 하는 과정을 포함한다. 따라서 레코드 단위로 commit() 을 하게 될 경우, record가 1000개일 경우 컨슈머는 브로커와 1000번의 통신을 하게 되는 것이다. 따라서 수동 커밋을 하는 경우에도 해당 방식은 잘 사용되지 않는다.
    
2. 비동기 오프셋 커밋 : poll() 메서드 호출 후, commitAsync() 메소드를 호출해서 커밋 처리를 한다. consumerRecords 들을 모두 처리하고 난 이후 해당 메소드를 호출하게 되면 백그라운드에서 커밋 작업을 수행하고, 바로 다음 consuemrRecords 들을 처리할 수 있게 된다.

****************Multi-Thread Consumer****************

카프카에서 컨슈머는 기본적으로는  1 Thread ↔ 1 Consumer 가 국룰이다.

만약 3개의 파티션을 가진 토픽을 구독하는 컨슈머 그룹에 컨슈머를 3개 두었을 때, 3개의 스레드를 가진 프로세스를 1개만 둘 것인지, 1개의 스레드를 가진 프로세스를 3개 둘 것 인지 선택해야 한다.

![스크린샷 2023-06-06 오후 4.46.54.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5a25dbfa-16e9-4716-9456-ae54fa21feed/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-06-06_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.46.54.png)

**Consumer Lag**

컨슈머 랙이란 ? 컨슈머가 토픽에서 데이터를 가져가는 속도보다, 토픽에 쌓이는 데이터의 량이 훨씬 더 많을 발생한다.

조금 더 전문적인 말로 요약하자면, 프로듀서가 토픽으로 보낸 가장 최신의 오프셋(LOG-END-OFFSET) 과 컨슈머가 현재 바라보고 있는 오프셋 (CURRENT-OFFSET) 간의 차이를 의미한다.

만약, LOG-END-OFFSET 이 4고 ,CURRENT-OFFSET 이 2 라면 Consumer Lag 는 2 다.
→ `LOG-END-OFFSET` - `CURRENT-OFFSET` = `Consumer LAG`

![스크린샷 2023-06-06 오후 4.54.23.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3b58ce4e-ce85-4180-a580-a7b4634ae9c5/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-06-06_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.54.23.png)

컨슈머 랙이 없는 실시간 데이터 처리를 위한 상태는 **오른쪽 상태**  가 이상적이다.

예를들어, 내비게이션 사용자 데이터를 전송하는 프로듀서가 있다고 가정해보자. 이 프로듀서는 휴가철이나 명절에 급격하게 프로듀서가 브로커로 보내는 데이터 량이 증가하게 되는데, 컨슈머 랙이 발생하게 될 것이다. 이 때 유일한 해결 방법은 파티션과 컨슈머의 개수를 늘리는 것이다.

**참고로, 컨슈머 렉은 파티션 별로 측정할 수 있다. (토픽 안에 파티션이 3개라면 3개가 측정 되며, 만약 해당 토픽을 구독하고 있는 컨슈머 그룹이 그 배수 만큼 증가하게 된다)**

😃 ex) 프로듀서의 전송량이 초당 10개, 컨슈머의 처리량이 초당 1개, 파티션이 20개라면 컨슈머 렉이 발생할까?
→ 발생하지 않는다. 아무리 프로듀서가 초당 10개의 데이터를 보내고 컨슈머가 1개의 데이터만 처리하고 있다고 해도, 컨슈머 그룹에 컨슈머를 파티션 개수만큼 맞춰놓는다면 그 컨슈머 그룹은 초당 20개 데이터를 처리할 수 있기 때문이다.

**************************************멱등성 프로듀서**************************************

멱등성이란, 여러 번 연산을 수행하더라도 동일한 결과를 나타내는 것을 의미한다. 따라서 카프카 프로듀서에서의 멱등성이란, 브로커로 여러 개의 동일한 레코드를 보내도 한 개의 레코드만 저장되는 것을 의미 할 것이다.

- At least once : 적어도 한 번 이상 전달
- At most once : 최대 한 번 전달
- Exactly once : 정확히 한 번 전달
    - Broker 에 전달하는 행위와 Offset Storage 에 저장하는 행위를 하나로 묶어 Atomic 하게 보장한다.

default 로는 At least once(적어도 한 번 전달) 인데, 이는 네트워크 오류 등의 이슈로 프로듀서가 동일한 데이터를 2번 이상 전달할 때 중복으로 전달될 가능성이 있음을 의미한다.

따라서, 프로듀서가 데이터를 중복으로 보내고 브로커가 중복 데이터를 적재하는 것을 방지하기 위해 0.11.0 버전 이후 부터는 `enable.idempotence` 라는 옵션을 제공하는데, 이를 true 로 설정하면 Exactly once (정확히 한번 전달) 을 활성화 할 수 있다.  (2.5.0 버전에서는 default 가 false 지만, 3.x.x 버전 부터는 true 이다.)

멱등성 보장을 위해서 아래 2개로 동일한 메시지인지 여부를 판단하게 된다.

- 프로듀서의 PID (프로듀서의 고유한 아이디)
- 레코드의 SID (레코드의 전달 번호 아이디)

위 2개를 보고 브로커는, 프로듀서가 동일한 데이터를 보냈더라도 한 개만 적재하게 된다.

**********************멱등성 보장이 필요한 사례**********************

producer 가 broker 로 레코드들을 전송하는 방법 중 건 바이 건이 아닌 batch 형태로 전송하게 될 경우,순서를 보장하기 위해  특정 KEY 를 지정하여 동일한 partition 으로 보냈다고 가정해보자.

batch 의 크기가 1000이면 1000 만큼 레코드를 쌓아서 partition 0 에 보내고, 또 1000 만큼 쌓아서 partition 0 에 보내게 된다면 총 0~2000 까지 데이터가 순서대로 전송이 될 것이다.

하지만 첫 번째 배치로 보내는 partition 0 으로 향하는 과정에서 오류가 발생해 데이터가 유실이 된다면?  0~1000 의 데이터를 잃어버리게 되는 것이다.  

이런 경우 멱등성 보장 옵션을 설정하여 partition 0 으로 향하는 send() 과정이 실패하게 될 경우 

하지만, 멱등성 프로듀서도 한계는 존재한다. 

프로듀서의 PID는 프로듀서의 라이프사이클 안에서만 고유한데, 만약 어떠한 이슈로 인해 프로듀서가 종료되었다가 다시 활성화 된 경우 PID 는 변경된다. 따라서 동일한 메시지를 전달하더라도 PID가 달라졌기 때문에 브로커에 동일한 데이터가 중복으로 저장될 수 있다.

또한, `enable.idempotence` 을 true 로 설정하게 되면 Exactly once 를 위해 몇 가지 옵션이 강제로 설정되는데 아래와 같다.

- retries 는 기본값으로 Integer.MAX_VALUE
- acks 옵션은 all (리더파티션에 적재 확인, 팔로워 파티션 확인 즉 ISR 확인)

위와 같이 설정되는 이유는 예상했듯이 브로커에 데이터가 한 번만 적재되는 것을 보장하기 위해 프로듀서가 브로커로 데이터를 보낼 때 결과값을 받아야 하기 때문이다.
→ 네트워크 통신도 많아지고 PID과 SID 또한 브로커의 메모리에 가지고있어야 한다. 부하가 좀 심해질 수 있다.
→ 다행히도 kafka 3.x.x 에서는 안정되었다고 하는데 그래도 부하는 생기기 마련이다.

→ 멱등성이 반드시 보장되어야 하는지 판단해서 사용하자.

**********************************************트랜잭션 프로듀서**********************************************

카프카에서 트랜잭션은 다수에 파티션에 데이터를 저장할 때, 의도한 모든 파티션에 데이터가 제대로 적재가 되거나, 안 될거면 아예 되지 않거나를 원할 때 사용한다. 즉 원자성을 보장하고 싶을 때 사용하는 것이다.

트랜잭션 프로듀서 파티션에 데이터를 적재 보낼 때 메시지 외에 트랜잭션의 시작과 끝을 표현하기 위한 트랜잭션 레코드를 한 개 더 보낸다.

- 트랜잭션 프로듀서를 동작하기 위해서는 `[transactional.id](http://transactional.id)` 를 설정해야 하는데, 반드시 고유해야 하기 때문에 UUID.randomUUID() 를 주로 이용한다.
- init → begin → commit  순으로 진행하고, begin 과 commit 사이에 전달할 메시지들 (또는 메시지) 을 send() 메소드를 통해 적재한다.

******************************트랜잭션 컨슈머******************************

트랜잭션 프로듀서가 보낸 레코드들이 토픽에 저장될 떄는 atomic 한 형태로 적재되어야 하기 때문에 토픽에 적재가 완료되었어도 컨슈머는 기다리게 된다. 바로 트랜잭션 프로듀서가 commit 을 날려 원자성이 보장된 상태로 토픽에 데이터가 정상적으로 적재가 되었다는 트랜잭션 레코드가 정상적으로 날라온 것을 확인하고 소비하게 된다.

- 트랜잭션의 격리레벨이 read_commiteed 로 변경해야 한다. (default는 read_uncommiteed 다)

### 카프카 스트림즈

카프카 스트림즈는 토픽에 적재된 데이터를 실시간으로 변환하여 다른 토픽에 적재하는 라이브러리다.

위 설명으로만 보면 카프카 스트림즈를 사용하지 않고도 프로듀서와 컨슈머를 적절히 활용하면 카프카 스트림즈 기능을 그대로 사용할 수 있지만, 카프카 스트림즈는 ******스트림 데이터 처리에 있어 필요한 다양한 기능을 제공하기 때문이다.******

- 소스 프로세서 : 가져올 데이터가 저장되어있는 토픽
- 스트림 프로세서 : 소스 프로세서에서 데이터를 가져와서 가공하고 변환하는 ㅍ로세서
- 싱크 프로세서 : 스트림 프로세서에서 가공한 데이터를 또 다른 토픽에 저장

### 카프카의 순서 보장

동일한 KEY 를 사용하는 경우, 파티션의 개수와 컨슈머의 개수가 동일한 경우 동일한 키에 대해서는 순서를 보장할 수 있다.

그러나, 파티션의 개수가 더 많고 컨슈머의 개수가 적은 경우에는 순서 보장이 어렵다.

그 이유는, 파티션의 개수가 더 많은 경우 한 개의 컨슈머는 두 개의 파티션에서 데이터를 가져가야 하는데 파티션이 컨슈머에게 주는 데이터 처리 속도가 네트워크 등의 이슈로 차이가 날 수 있기 때문이다.

즉, 순서를 보장하고 싶은 경우 파티션과 컨슈머의 개수를 1:1 로 맞추도록 하자.
