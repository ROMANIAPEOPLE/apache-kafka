### apache kafka 기본 용어 정리
- Zookeeper : 아파치 프로젝트 Application 으로 kafka 의 metaData 관리 및 Broker 의 health check (정상 상태 점검)을 담당
  - 토픽 생성, 제거 및 브로커의 추가와 제거 등 변경사항을 kafka 에 알림
  - 주키퍼 서버의 클러스터를 주키퍼 앙상블 (Zookeeper Ensemble) 이라고 하며, Ensemble 이 몇 대로 구성이 되어있는지에 따라 주키퍼의 동작 여부가 결정된다.
  - 만약 Ensemble 이 5대로 구성되어 있고, 2대가 장애가 발생한다면 정상 동작한다. (즉, 과반 이상이 정상 상태여야 정상적으로 동작하므로 주키퍼는 홀수 개의 서버로 동작하게 설계)
- Broker : Kafka Application 이 설치된 서버 또는 노드를 의미한다.
- Producer : 카프카로 메시지를 보내는 역할을 하는 클라이언트
- Consumer : 카프카에서 메시지를 꺼내가는 역할을 하는 클라이언트
- Topic : 카프카는 메시지 피드들을 토픽으로 분리하고, 각 토픽의 이름은 카프카 내에서 고유
- Partition : 병렬처리와 고가용성을 위해 하나의 토픽을 여러 개로 나눈 것을 의미
- Segment : 프로듀서가 전송한 실제 메시지가 로컬 디스크에 저장되는 파일
- record (=message) : 프로듀서가 브로커로 전송하거나 컨슈머가 읽어가는 데이터 조각

### 빅데이터 파이프라인에 적합한 Kafka 의 특징
- 높은 처리량 : 카프카는 프로듀서가 브로커로 데이터를 보낼 때와 브로커로부터 컨슈머가 데이터를 가져갈 때 모두 묶어서 전송하는데, 이는 많은 양의 데이터를 송수신할 때 맺어지는 네트워크 비용이 크기 때문이다. 또한 파티션 단위를 통해 동일 목적의 데이터를 여러 파티션에 분배하여 높은 가용성과 데이터의 병렬 처리를 지원한다.
- 확장성 : 처리량에 따라 Broker 의 스펙을 scale out 또는 scale in 할 수 있다.
- 영속성 : 카프카는 다른 메시징 큐와 다르게 파일 시스템에 저장하기 때문에 컨슈머가 특정 레코드를 소비한다고 데이터가 삭제되지 않는다. 또한 브로커에 장애가 발생하더라도 파일시스템에 저장되어 있기 때문에 영속이 유지된다.
- 고가용성 : 3개 이상의 서버들로 운영되는 카프카 클러스터 (다수의 브로커) 는 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다. 하나의 브로커에 장애가 발생하면 다른 브로커에 데이터가 복제되어 있어서 고가용성을 유지할 수 있다.

### Kafka 의 영속성 - 파일시스템(디스크)
파일시스템. 즉 disk 에 로그가 저장되는데 **왜** 빠를까?
1. 프로듀서가 브로커로 메시지를 발행하는 과정
<img width="702" alt="스크린샷 2023-10-08 오후 7 39 14" src="https://github.com/kihyuk-jeong/learn-kit/assets/39195377/63e89527-b15e-42b2-8a76-c6ce6db943c7">

- 프로듀서 애플리케이션에서 메시지를 발행한다.
- 브로커로 가기 전, 프로듀서의 Accumulator (어큐뮬레이터) 에 batch 형태로 메시지가 쌓이게 된다.
- 사용자가 설정한 `batch.size` 또는 `linger.ms` 옵션의 수치에 따라 브로커로 메시지를 보낸다.
- batch_size 를 50으로 설정했다면, 프로듀서가 발행한 메시지가 어큐뮬레이터에 50 만큼 쌓이게 되었을 때 브로커로 메시지들을 전달하는 것이다.
- 다만 bath_size 옵션만 사용하게 된다면, 메시지 발행량이 적은 애플리케이션은 브로커로 전달되는 주기가 매우 적거나 거의 전달되지 않을 가능성이 존재하기 때문이다.
- 이렇게 건바이건으로 전달하지 않고 배치 형태로 묶어서 전달하는 이유는 broker 로 메시지를 발행하는 과정에서 TCP/IP 프로토콜을 사용하기 때문이다. (즉, 네트워크 연결에 있어서 효율을 증진시키기 위함이다)
- 추가로, Accumulator 의 사이즈는 프로듀서의 설정 옵션 중 하나인 `buffer.memory` 에 의해서 결정된다.

  **Accumulator 의 buffer.memory 설정**
- Accumulator 에 임시 저장되는 레코드의 크기는 `buffer.memory` 설정에 따른다.
- 만약 Accumulator 에서 broker 로 전송하는 속도가 Accumulator 에 메시지들이 쌓이는 속도보다 느려지게 되면 문제가 발생한다.
- 예를들어 `buffer.memory` 의 설정을 5000으로 하고, Accumulator 에서 브로커로 이동하는 속도가 저하됨에 따라 Accumulator 에 5000보다 많은 데이터들이 쌓이게 되면 Accumulator 로 데이터를 쌓는 작업이 block 이 된다. 
(즉, 프로듀서 애플리케이션에서 브로커로 메시지를 발행하는 작업이 block 이 된다.)
- 만약 block 되는 시간이 사용자가 설정한 max.block.ms 의 수치를 초과하게 된다면 exception 이 발생한다.

2. 브로커가 디스크에 레코드를 저장하는 과정
<img width="625" alt="스크린샷 2023-10-08 오후 8 22 15" src="https://github.com/kihyuk-jeong/learn-kit/assets/39195377/cb7db6b6-0def-4819-85c3-c0bfbd742e91">
- 프로듀서에서 발행한 메시지가 브로커에 도착하면 브로커는 레코드를 로그 파일 시스템에 쓰기 작업을 시작한다.
- 실제로 브로커가 레코드(=메시지) 들을 파일시스템에 쓸 때, OS 의 페이지 캐시를 활용하게 되는데, 곧바로 디스크에 쓰기 작업을 하는 것이 아니라 페이지 캐시에 저장을 한 후 운영체제가 이후 더 효율적인 시점에 캐시된 데이터를 물리 디스크에 기록한다.
- 다만 브로커가 프로듀서로부터 메시지를 받은 즉시 디스크에 기록하는 것이 아닌 OS 페이지 캐시에 기록된 데이터를 일정 주기에 맞춰 기록하는 메커니즘이므로 페이지 캐시에서 disk 로 기록하는 과정에서 장애가 발생한다면, 데이터가 유실될 가능성이 존재한다.
  (따라서 replica factor 를 구성해야 한다.)

3. zero-copy
disk 에 저장된 레코드들을 consumer 애플리케이션에서 소비할 때 또는 다른 broker 에 복제할 때 zero-copy 기술을 활용하여 높은 처리량을 지원한다.
- zero-copy 를 사용하면 일반적인 데이터 전송 과정과 다르게 데이터를 디스크에서 직접 네트워크로 전송할 수 있다. 이로 인해 CPU 사용량과 지연 시간이 크게 줄어들며, 전송 속도와 처리량이 크게 향상된다.

### 브로커의 역할
- 컨트롤러 : 카프카 클러스터 내 다수의 브로커 중 한 대가 컨트롤러 역할을 하며, 컨트롤러는 파티션의 재할당 및 파티션의 리더 선출을 담당한다.
- 레코드 관리 : 카프카는 다른 MQ 와 다르게 컨슈머가 데이터를 가져가도 삭제되지 않으며, 로그 파일의 형태로 디스크에 저장된다. 카프카의 데이터를 삭제할 수 있는건 오로지 브로커 뿐이며 브로커 조차도 특정 데이터를 콕 찝어서 삭제할 수 없다. 레코드를 압축해서 용량을 확보하거나, 일정 기간이 경과한 데이터만 삭제가 가능할 뿐이다.
- 컨슈머의 오프셋 저장 : 컨슈머 그룹은 토픽이 특정 파티션으로부터 데이터를 가져가서 처리하고, 이 파티션의 어느 레코드 까지 가져갔는지 확인하기 위해 오프셋을 커밋한다. 커밋한 오프셋은 __consumer_offsets 라는 토픽에 저장되며, 이 토픽은 internal topic 으로 사용자가 직접 이 토픽을 확인하는 일은 극히 드물다.
- 그룹 코디네이터 : 그룹 코디네이터는 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배하는 역할을 한다. 예를 들어, 컨슈머 그룹 내부에 있는 컨슈머가 문제가 생겨 동작하지 않게 되면 해당 컨슈머에게 데이터를 주던 파티션을 정상적인 컨슈머와 연결시켜 재할당 한다. 이를 파티션 리밸런싱이라고 부른다.

  **데이터 삭제/압축**
  - cleanup.policy=delete / cleanup.policy=compact
  - retention.ms : 세그먼트(레코드의 로그파일) 을 보유할 최대 기간. 기본값은 7일로, 이 기간이 지나면 삭제된다.
  - retention.bytes : 파티션당 로그 적재 바이트 값.
  - lop.retention.check.interval.ms : 세그먼트가 삭제 영역에 들어왔는지 확인하는 간격. 기본 값은 5분
  - 위 설정 값에 따라 데이터가 삭제되거나 압축되는데, active segment 라고 현재 최신 레코드들이 기록되고 있는 세그먼트는 삭제나 압축 대싱에 포함되지 않는다.
  - 압축(compact) 는 동일한 KEY 가 존재하는 경우 오래된 KEY 에 해당하는 레코드를 제거하는 것이다. (따라서 KEY 가 없는 레코드들은 압축 대상이 아니다.)

  **복제**

  <img width="678" alt="스크린샷 2023-10-08 오후 9 04 04" src="https://github.com/kihyuk-jeong/learn-kit/assets/39195377/4cdd2024-8022-42e4-85c0-84a4582ddc83">
    - 카프카의 강력한 장점 중 하나인 HA 를 위해 replica factor 를 구성해야 한다.
    - 예를들어 브로커를 3개로 구성했다면 replica factor 는 총 3개로, 동일한 데이터가 총 3개의 브로커에 저장되는 것이다. 따라서 1GB 의 데이터를 프로듀서가 발행한다면, 1GB가 아닌 총 3GB (x replica factor 의 수) 가 되는 것이다.
    - 리더 파티션이 하나의 브로커에 몰리지 않도록 잘 분배해야 한다.


### ISR (In-Sync-Replicas)
replica factor 를 구성한 환경에서 리더 파티션과 팔로워 파티션의 오프셋이 동일한 상태 (=즉, 모두 싱크가 맞는 상태) 를 의미한다.
반대 개념으로 싱크가 맞지 않는 상태를 OSR 이라고 한다.
- 프로듀서가 데이터를 발행하면 리더 파티션에 데이터가 쌓이고, 팔로워 파티션은 리더 파티션의 offset 과 자신의 offset 을 비교해서 두 값이 다르면 복제를 시작하게 되는 구조이다.
- 이 때 만약 리더 파티션을 가진 브로커에 장애가 발생한다면 복제 파티션 중 하나가 리더 파티션으로 승격되는데, 이 때 미처 복제하지 못한 데이터는 유실되고 만다. (아래 옵션에 따라 유실을 감수 할 것인지 결정한다.)
  - unclean.leader.election.enable=true  : 유실을 감수한다. 복제가 안된 팔로워 파티션을 리더로 승급시킨다.
  - unclean.leader.election.enable=false : 유실을 감수하지 않는다. 해당 브로커가 복구될 때까지 중단한다.
- 해당 브로커에 대한 프로듀서의 쓰기와 컨슈머의 읽기가 모두 중단된다. 이렇게 되면 중단 되는 동안 카프카로 데이터가 전달되지 않아 데이터 유실이 발생할 수 있다. (dead-letter-queue 와 같은 DR 클러스터 구성 필요)

### ISR 과 OSR 을 판단하는 기준
replica.lag.time.max.ms 옵션을 기준으로 판단한다.
- 팔로워가 리더로 Fetch 요청을 보내는 Interval 을 체크한다.
- 예를들어, 해당 옵션이 10000 이라면 팔로워 파티션이 리더 파티션으로 Fetch 요청을 10000ms 내에 보내서 리더로 부터 데이터를 가져오면 ISR 로 판단하고, 10000ms 내에 처리되지 않으면 ISR 목록에서 제거되어 OSR 상태가 된다.

<img width="705" alt="스크린샷 2023-10-08 오후 9 27 42" src="https://github.com/kihyuk-jeong/learn-kit/assets/39195377/9e065d5a-6cbe-46ea-a47c-bc232630924b">

- 리더 파티션과 팔로워 파티션이 ISR 상태인 offset 을 High Water Mark 라고 부른다. 즉, 리더 파티션을 기준으로 팔로워 파티션이 리더 파티션을 어디까지 복제 했는지 나타내는 offset 이다.
- consumer group 은 HWM 까지만 소비할 수 있다.
- 즉, HWM 은 consumer group 에서 소비한 가장 최신의 Offset 이다.
- Log End Offset (=LEO) 는 producer 가 메시지를 발행했을 때, 적재 될 Offset 위치를 가르킨다.

### acks 옵션
카프카 프로듀서의 acks 옵션은 0,1,-1(또는 all) 값을 가질 수 있다.

이 옵션을 통해 프로듀서가 전송한 데이터가 카프카 플러스터에 얼마나 신뢰성 높게 저장할지 지정할 수 있다. 이 옵션에 따라 성능이 달라질 수 있다. (단, replica factor 가 1인 경우는 성능 변화가 크지 않다.) 

그렇지만 실무에서는 대부분 replica factor 를 2 이상으로 두고 운영을 하기 때문에, acks 옵션에 따른 각각의 성능 차이를 잘 알아야 한다.

- acks 0 : 프로듀서가 리더 파티션으로 데이터를 전송했을 때 무조건 전송이 완료 되었다고 생각하는 설정이다. 즉, 정상적으로 저장이 되었는지 확인하지 않는다. 기본적으로 프로듀서는 리더 파티션에 데이터가 저장이 되면 몇 번째 오프셋에 저장되었는지 리턴하는데 이것을 리턴 받지 않겠다는 뜻이다. 
→ 데이터가 일부 유실되더라도 상관 없는 경우 사용한다. (로그?)

- acks 1 : 프로듀서가 보낸 데이터가 리더 파티션에만 정상적으로 적재 되었는지 확인한다. 리더 파티션에 정상적으로 적재되지 않았다면, 리더 파티션에 적재될 때까지 재시도할 수 있다. 리더 파티션에 데이터가 저장되었다는 것은 파일시스템이 디스크에 저장되었다는 의미로, 이러한 응답을 받기 까지는 시간이 좀 걸린다. (0보다 오래 걸림)
→ 단, 리더 파티션만 확인하기 때문에 팔로워 파티션에는 아직 데이터가 동기화 되지 않았을수도 있기 때문에.. 리더 파티션의 브로커에 장애가 발생헀을때 아직 동기화되지 못한 팔로워 파티션이 리더로 승급할 경우, 일부 데이터가 유실될 수 있다. (단, 매우 드물기 때문에 대부분 실무에서는 1로 설정한다.)

- acks -1 (또는 all 이라고도 부른다) : 리더/팔로워 모두 정상적으로 적재 되었는지 확인한다. acks 를 all 로 설정한 경우 토픽 단위로 설정이 가능한 `min.insync.replicas` 옵션에 따라 데이터의 안정성이 달라진다. 이 설정은, 만약 replica factor 가 2인 카프카 클러스터의 경우, 총 3개의 브로커를 가지게 되는데, 이 설정을 2로 할 경우 리더1개와 팔로워 1개만 all(-1) 에 대한 설정을 적용받는다. 대부분 2로 설정하면 된다. 모든 브로커서버가 박살날 가능성은 로또 맞을 확률과 비슷하다. (하나의 리더와 하나의 팔로워만 확인 해도 신뢰도 높은 애플리케이션을 만들 수 있다는 뜻)
→ 신뢰도는 매우 높고, 처리 속도는 낮다. (데이터 처리량이 너무 낮아서 사용하지 못하는 수준임)
→ 절대 절대 유실되어서는 안되는 데이터 + 처리량이 매우 낮아도 아무런 상관 없는 경우 사용.
→ at least once (최소 한 번) 전송을 보장.


### 레코드
- 타임스탬프 : 기본적으로  ProducerRecord의 createTime 이 들어가지만 (레코드가 생성된 시간) 설정을 변경하여 브로커에 레코드가 적재된 시간인 LogAppendTime 으로 설정할수도 있다.
(message.timestamp.type) 을 사용한다.
- 오프셋 : 오프셋은 프로듀서가 데이터를 생성한 직후에는 존재하지 않으며, 프로듀서가 전송한 레코드가 브로커에 적재될 때 생성되는 것이다. 오프셋은 0부터 1씩 증가하게 된다. 각 메시지는 파티션별로 고유한 오프셋을 가지므로, 컨슈머에서 중복 처리를 방지하기 위한 목적으로도 사용한다.
- 헤더 : key/value 데이터를 추가할 수 있으며 레코드의 스키마 버전이나 포캣과 같이 데이터 처리에 참고할만한 정보를 담아서 사용할 수 있다. (0.11 버전부터 지원)
- KEY : 카프카는 프로듀서가 토픽으로 메시지를 전달할 때 토픽 내 어느 파티션에 메시지가 저장되는지 지정할 수 있는데, 바로 KEY를 사용해서 특정 파티션에 저장하는 것이다. KEY 값은 필수값이 아니며 지정하지 않을 경우 null 로 설정되며 라운드 로빈 방식으로 각 파티션으로 메시지가 분배된다. null 이 아닌 메시지 키는 해쉬값에 의해서 특정 파티션에 매핑되어 전달된다.
- VALUE : **실질적으로 처리할 데이터로,** 제네릭형태로 사용자의 지정 타입으로 메시지가 전달된다. 컨슈머 입장에서는 어떤 타입으로 메시지가 토픽에 저장됐는지 모르기 때문에, 컨슈머 로직에 반드시 미리 역직렬화 포맷을 지정해야 한다.

### 리밸런싱
컨슈머 그룹으로 이루어진 컨슈머들 중 일부 컨슈머에 장애가 발생하면 장애가 발생한 컨슈머에게 데이터를 보내던 파티션은 재빨리 정상적인 컨슈머에게 데이터를 주도록 해야하는데, 이 과정을 리밸런싱 이라고 부른다. (참고로 컨슈머 리밸런싱 과정에서 컨슈머는 멈춘다.)

- 컨슈머가 추가될 때
- 컨슈머가 제외 (장애로 인해) 될 때
- 파티션이 추가될 때 (파티션은 추가만 가능하고 제거할수는 없음)
- 컨슈머가 Topic 구독을 변경할 때

리밸런싱은 위 상황에서 발생하며, 데이터 처리 중 발생한 리밸런싱에 대응하는 코드를 작성해야 한다. (리밸런싱 리스너가 존재한다.)

onPartitionAssgined() : 리밸런스가 끝난 뒤에, 파티션 할당이 완료 되었을 때 호출됨.

onPartitionRevoked() : 리밸런스가 시작되기 직전에 호출하는 메소드로, 리밸런스가 시작 되기 직전에 처리된 record 를 기준으로 commit 을 해야 하기 때문에, 해당 메소드에 커밋을 구현하여 처리할 수 있다.

  **리밸런싱이 발생했을 떄 Conusmer Group 내에서 일어나는 일**
  - 컨슈머 그룹 내에서 consumer 의 리더를 선출하고, 그 리더가 각 컨슈머들에게 파티션을 할당한다.
  - 할당이 완료 된 리더는 그룹 코디네이터(브로커 중 하나) 에게 할당 결과를 알리고, 그룹 코디네이터는 주키퍼에게 변경된 상태를 전달한다.
  - 브로커가 직접 컨슈머를 할당하지 않는 이유는 간단하다. 수 많은 토픽과 컨슈머 그룹을 브로커가 모두 관리하기는 어렵기 때문이다.
    
### Consumer 주요 옵션
- [group.id](http://group.id) : 컨슈머 그룹 아이디를 지정한다. subscribe() 메서드로 토픽을 구독하여 사용할 때는 이 옵션을 필수로 넣어야 한다.
- auto.offset.reset : 컨슈머 그룹이 특정 파티션을 읽을 때, offset 이 없는 경우 어디부터 읽을지 선택하는 옵션이다. (한 번도 commit 되지 않은 파티션에는 offset이 없다.) 기본 옵션은 latest 이다.
    - latest : 가장 높은(가장 최근) 오프셋 부터 읽기 시작
    - earliest : 가장 낮은 (가장 오래전에 넣은) 오프셋 부터 읽기 시작
    - none : 커밋한 기록을 찾아보고, 기록이 없으면 오류 반환 (사용 거의 안 함)
    - 한 번이라도 커밋된 이력이 존재한다면, 위 옵션들은 전부 무시된다.
- enable.auto.commit : true/false 로 설정할 수 있으며, 특정 시간마다 자동으로 커밋을 해서 offset 을 지정하는 옵션이다. (default : true)
- [auto.commit.interval.ms](http://auto.commit.interval.ms) : enable.auto.commit 을 true로 설정한 경우 몇 초 마다 commit 을 할 것 인지 설정하는 옵션이다. (default : 5000ms = 5초)
- max.poll.records : poll() 메서드를 통해 반환되는 레코드의 개수를 지정하며, 기본값은 500이다. 만약 더 많은 데이터를 처리하고 싶다면 숫자를 늘려라.
- [session.timeout.ms](http://session.timeout.ms) : 컨슈머가 브로커와 연결이 끊기는 최대 시간이다. 기본값은 10000 (10초)
- [heartbeat.interval.ms](http://heartbeat.interval.ms) : 하트비트를 전송하는 시간 간격이다. 기본값은 3000 (3초)
→ 즉, 하트비트를 브로커에 3초 간격으로 계속해서 보내다가, 하트비트가 오지 않으면 [session.timeout.ms](http://session.timeout.ms) 초동안 기다렸다가, 그래도 안 오면 익셉션 발생 → 리밸런싱이 이루어짐
- [max.poll.interval.ms](http://max.poll.interval.ms) : poll() 메소드를 호출하는 간격의 최대 시간. 기본값은 5분이다.
→ 5분이 지났는대도 poll() 메소드를 호출하지 않는다면 리밸런싱 대상.
- isolation.level : 트랜잭션 프로듀서가 레코드를 트랜잭션 단위로 보낼 경우 사용 (잘 사용하지 않음)

### Consumer lag
컨슈머가 토픽에서 데이터를 가져가는 속도보다, 토픽에 쌓이는 데이터의 량이 훨씬 더 많을 발생한다.

조금 더 전문적인 말로 요약하자면, 프로듀서가 토픽으로 보낸 가장 최신의 오프셋(LOG-END-OFFSET) 과 컨슈머가 현재 바라보고 있는 오프셋 (CURRENT-OFFSET) 간의 차이를 의미한다.

만약, LOG-END-OFFSET 이 4고 ,CURRENT-OFFSET 이 2 라면 Consumer Lag 는 2 다.
→ `LOG-END-OFFSET` - `CURRENT-OFFSET` = `Consumer LAG`
<img width="681" alt="스크린샷 2023-10-08 오후 9 51 13" src="https://github.com/kihyuk-jeong/learn-kit/assets/39195377/3586e782-231d-4b31-b3c4-324bd3c6154e">

컨슈머 렉이 없는 실시간 데이터 처리를 위한 상태는 **오른쪽 상태**  가 이상적이다.

예를들어, 내비게이션 사용자 데이터를 전송하는 프로듀서가 있다고 가정해보자. 이 프로듀서는 휴가철이나 명절에 급격하게 프로듀서가 브로커로 보내는 데이터 량이 증가하게 되는데, 컨슈머 랙이 발생하게 될 것이다. 이 때 유일한 해결 방법은 파티션과 컨슈머의 개수를 늘리는 것이다.

**참고로, 컨슈머 렉은 파티션 별로 측정할 수 있다. (토픽 안에 파티션이 3개라면 3개가 측정 되며, 만약 해당 토픽을 구독하고 있는 컨슈머 그룹이 그 배수 만큼 증가하게 된다)**

ex) 프로듀서의 전송량이 초당 10개, 컨슈머의 처리량이 초당 1개, 파티션이 20개라면 컨슈머 렉이 발생할까?
→ 발생하지 않는다. 아무리 프로듀서가 초당 10개의 데이터를 보내고 컨슈머가 1개의 데이터만 처리하고 있다고 해도, 컨슈머 그룹에 컨슈머를 파티션 개수만큼 맞춰놓는다면 그 컨슈머 그룹은 초당 20개 데이터를 처리할 수 있기 때문이다.


### 프로듀서의 내부 구조
<img width="608" alt="스크린샷 2023-10-09 오후 5 15 21" src="https://github.com/kihyuk-jeong/learn-kit/assets/39195377/bcb8d322-6767-45a3-9ca1-d871586127bd">

- ProducerRecord : 프로듀서에서 생성하는 레코드. 참고로 오프셋은 미포함 (offset 은 메시지가 파티션에 적재된 후 생성됨)
- send() : 레코드(=메시지) 를 브로커로 전송하는 메소드
- Partitioner() : 어느 파티션으로 전송할지 지정한다.
- Accumlator : 프로듀서에서 브로커로(카프카 클러스터) 로 데이터를 전송하게 되면 TCP 통신을 해야하는데, 매번 TCP 통신을 위한 3-way-handshake 와 4-way-handshake 의 부담을 덜어주기 위해 Batch 로 한 번에 데이터를 묶어서 전송할 때 사용한다.

### Partitioner 종류
1. **message 의 KEY 가 존재하지 않는 경우**
  - RoundRobinPartitioner : ProducerRecord 가 드렁오는 대로 파티션을 순회하며 전송한다. 어큐뮬레이터에 데이터가 적절히 쌓이기도 전에 라운드로빈 방식으로 각 파티션에 데이터를 적재하기 때문에 네트워크 통신에 있어서 효율이 떨어질 수 있다.
  - UniformStickeyPartitioner : kafka 2.5.0 버전 이후부터 default 파티셔너로 지정되었으며, ProducerRecord 를 어큐뮬레이터의 memory buffer 가 채워질 때 까지 기다렸다가 보내기 때문에 네트워크 통신에 필요한 오버헤드를 최소화 할 수 있다.

2. **message 의 KEY 가 존재하는 경우**
  - KEY가 존재하는 경우 위 파티셔너 정책들과는 무관하게 KEY 를 hash 값으로 변환한 후, 그 값을 기준으로 파티션에 데이터를 보낸다.
  - 정확한 공식은 KEY 의 hashCode() 를 구하고, 그 값을 해당 토픽의 파티션 총 개수로 modulo 연산 (%) 한다.
  - `partition = key.hashCode() % numberOfPartitions`
  - 위와 같은 공식을 사용하기 때문에 운영하는 도중에 파티션의 개수를 추가하게 되면 KEY 와 파티션 번호의 매칭이 깨지데 된다.
  
### kafka 순서보장
- 동일한 KEY 를 사용하는 경우, 파티션의 개수와 컨슈머의 개수가 동일한 경우 동일한 키에 대해서는 순서를 보장할 수 있다.
- 파티션의 개수가 더 많고 컨슈머의 개수가 적은 경우에는 순서 보장이 어렵다. 그 이유는 파티션의 개수가 더 많은 경우 한 개의 컨슈머는 두 개의 파티션에서 데이터를 가져가야 하는데 파티션이 컨슈머에게 주는 데이터 처리 속도가 네트워크 등의 이슈로 차이가 날 수 있기 때문이다.
- 즉, 순서를 보장하고 싶은 경우 파티션과 컨슈머의 개수를 1:1 로 맞추도록 하자.
- 배치 사이즈로 프로듀서에서 브로커에 레코드를 적재하는 경우에도 순서 보장에 문제가 생길수도 있다. 첫 번째 배치, 두 번째 배치에 모두 순서대로 레코드가 적재되어 있을 때, 첫 번째 배치가 실패하고 두 번째 배치가 파티션에 적재하는것이 성공한다면, 순서 보장이 깨지게 된다. (멱등성 보장 필요)

### 멱등성 프로듀서

### Delivery Semantics
